#!/usr/bin/env python3
"""
åˆ†æå¯¦é©—ä¸­æ¯å€‹ä¸–ä»£çš„æ¨¹çµæ§‹çµ±è¨ˆ

è¼¸å‡ºæ¯å€‹ä¸–ä»£çš„ï¼š
- æ¨¹æ·±åº¦ï¼ˆå¹³å‡ã€æœ€å°ã€æœ€å¤§ï¼‰
- ç¯€é»æ•¸ï¼ˆå¹³å‡ã€æœ€å°ã€æœ€å¤§ï¼‰
"""

import argparse
import pickle
import json
from pathlib import Path
import sys

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from deap import base, creator, gp

# è¨­ç½® DEAP creatorï¼ˆç”¨æ–¼ pickle ååºåˆ—åŒ–ï¼‰
if not hasattr(creator, "FitnessMax"):
    creator.create("FitnessMax", base.Fitness, weights=(1.0,))
if not hasattr(creator, "Individual"):
    creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)


def analyze_generation(pkl_file: Path) -> dict:
    """
    åˆ†æå–®ä¸€ä¸–ä»£çš„æ¨¹çµæ§‹
    
    Args:
        pkl_file: generation_XXX.pkl æ–‡ä»¶è·¯å¾‘
        
    Returns:
        dict: çµ±è¨ˆè³‡è¨Š
    """
    # è¼‰å…¥æ—ç¾¤
    with open(pkl_file, 'rb') as f:
        data = pickle.load(f)
    
    # æå–æ—ç¾¤
    if isinstance(data, dict) and 'population' in data:
        population = data['population']
    else:
        population = data
    
    # æå–ä¸–ä»£ç·¨è™Ÿ
    gen_num = int(pkl_file.stem.split('_')[1])
    
    # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
    depths = [ind.height for ind in population]
    sizes = [len(ind) for ind in population]
    
    stats = {
        'generation': gen_num,
        'population_size': len(population),
        'depth': {
            'mean': sum(depths) / len(depths),
            'min': min(depths),
            'max': max(depths)
        },
        'nodes': {
            'mean': sum(sizes) / len(sizes),
            'min': min(sizes),
            'max': max(sizes)
        }
    }
    
    return stats


def main():
    parser = argparse.ArgumentParser(description='åˆ†æå¯¦é©—ä¸­æ¯å€‹ä¸–ä»£çš„æ¨¹çµæ§‹çµ±è¨ˆ')
    parser.add_argument('--exp_dir', type=str, required=True,
                       help='å¯¦é©—ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--output', type=str, default=None,
                       help='è¼¸å‡º JSON æ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰')
    
    args = parser.parse_args()
    
    # è§£æè·¯å¾‘
    exp_dir = Path(args.exp_dir)
    generations_dir = exp_dir / 'generations'
    
    if not generations_dir.exists():
        print(f"âœ— æ‰¾ä¸åˆ° generations ç›®éŒ„: {generations_dir}")
        return 1
    
    # ç²å–æ‰€æœ‰ pkl æ–‡ä»¶
    pkl_files = sorted(generations_dir.glob('generation_*.pkl'))
    
    if not pkl_files:
        print(f"âœ— åœ¨ {generations_dir} ä¸­æ‰¾ä¸åˆ° generation_*.pkl æ–‡ä»¶")
        return 1
    
    print("=" * 80)
    print("ğŸŒ² æ¨¹çµæ§‹çµ±è¨ˆåˆ†æ")
    print("=" * 80)
    print()
    print(f"å¯¦é©—ç›®éŒ„: {exp_dir}")
    print(f"ä¸–ä»£æ•¸: {len(pkl_files)}")
    print()
    
    # åˆ†ææ¯å€‹ä¸–ä»£
    all_stats = []
    
    print("æ­£åœ¨åˆ†æ...")
    print()
    
    for pkl_file in pkl_files:
        stats = analyze_generation(pkl_file)
        all_stats.append(stats)
        
        # è¼¸å‡ºåˆ°çµ‚ç«¯
        print(f"Generation {stats['generation']:3d}:")
        print(f"  æ—ç¾¤å¤§å°: {stats['population_size']}")
        print(f"  æ¨¹æ·±åº¦:   å¹³å‡={stats['depth']['mean']:6.2f}  "
              f"æœ€å°={stats['depth']['min']:3d}  æœ€å¤§={stats['depth']['max']:3d}")
        print(f"  ç¯€é»æ•¸:   å¹³å‡={stats['nodes']['mean']:6.2f}  "
              f"æœ€å°={stats['nodes']['min']:3d}  æœ€å¤§={stats['nodes']['max']:3d}")
        print()
    
    # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
    all_depths_mean = [s['depth']['mean'] for s in all_stats]
    all_nodes_mean = [s['nodes']['mean'] for s in all_stats]
    
    print("=" * 80)
    print("ğŸ“Š ç¸½é«”çµ±è¨ˆ")
    print("=" * 80)
    print()
    print(f"å¹³å‡æ¨¹æ·±åº¦ç¯„åœ: {min(all_depths_mean):.2f} - {max(all_depths_mean):.2f}")
    print(f"å¹³å‡ç¯€é»æ•¸ç¯„åœ: {min(all_nodes_mean):.2f} - {max(all_nodes_mean):.2f}")
    print()
    
    # å„²å­˜åˆ° JSONï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = exp_dir / 'tree_structure_stats.json'
    
    result = {
        'experiment': exp_dir.name,
        'experiment_path': str(exp_dir),
        'total_generations': len(all_stats),
        'statistics': all_stats
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… çµæœå·²å„²å­˜åˆ°: {output_file}")
    print()
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
