"""
å°‹æ‰¾æœ€ä½³çš„ Cluster æ•¸é‡ (k)

ä½¿ç”¨ Silhouette Score è©•ä¼°ä¸åŒ k å€¼çš„èšé¡å“è³ª
è¼¸å…¥ï¼šgeneration.pkl æ–‡ä»¶è·¯å¾‘
è¼¸å‡ºï¼šæœ€ä½³ k å€¼å’Œå¯è¦–åŒ–åœ–è¡¨
"""

import sys
from pathlib import Path
import dill
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.cluster import KMeans
import pandas as pd

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from deap import base, creator, gp, tools
from gp_quant.gp.operators import pset
from gp_quant.similarity import SimilarityMatrix, ParallelSimilarityMatrix

# åˆå§‹åŒ– DEAP creator
if not hasattr(creator, "FitnessMax"):
    creator.create("FitnessMax", base.Fitness, weights=(1.0,))
if not hasattr(creator, "Individual"):
    creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)

# è¨­ç½®ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def load_generation(pkl_path):
    """è¼‰å…¥ generation.pkl"""
    print(f"ğŸ“‚ è¼‰å…¥æ–‡ä»¶: {pkl_path}")
    
    with open(pkl_path, 'rb') as f:
        data = dill.load(f)
    
    generation = data['generation']
    population = data['population']
    
    print(f"   âœ“ Generation: {generation}")
    print(f"   âœ“ Population size: {len(population)}")
    
    return data


def compute_similarity_matrix(population, use_parallel=True):
    """è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£"""
    print(f"\nğŸ”¬ è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£...")
    print(f"   Population size: {len(population)}")
    
    if use_parallel and len(population) >= 200:
        print(f"   ä½¿ç”¨ä¸¦è¡Œè¨ˆç®—ï¼ˆ2 æ ¸å¿ƒï¼‰...")
        sim_matrix = ParallelSimilarityMatrix(population, n_workers=2)
        similarity_matrix = sim_matrix.compute(show_progress=True)
    else:
        print(f"   ä½¿ç”¨åºåˆ—è¨ˆç®—...")
        sim_matrix = SimilarityMatrix(population)
        similarity_matrix = sim_matrix.compute(show_progress=True)
    
    print(f"   âœ“ ç›¸ä¼¼åº¦çŸ©é™£å½¢ç‹€: {similarity_matrix.shape}")
    print(f"   âœ“ å¹³å‡ç›¸ä¼¼åº¦: {sim_matrix.get_average_similarity():.4f}")
    print(f"   âœ“ å¤šæ¨£æ€§åˆ†æ•¸: {sim_matrix.get_diversity_score():.4f}")
    
    return similarity_matrix


def evaluate_k_range(similarity_matrix, k_min=2, k_max=50):
    """è©•ä¼°ä¸åŒ k å€¼çš„èšé¡å“è³ª"""
    print(f"\nğŸ“Š è©•ä¼° k å€¼ç¯„åœ: [{k_min}, {k_max}]")
    
    # è½‰æ›ç‚ºè·é›¢çŸ©é™£
    distance_matrix = 1.0 - similarity_matrix
    np.fill_diagonal(distance_matrix, 0.0)
    
    results = []
    
    for k in range(k_min, k_max + 1):
        if k >= len(similarity_matrix):
            print(f"   âš ï¸  k={k} è¶…é population sizeï¼Œè·³é")
            break
        
        print(f"   æ¸¬è©¦ k={k}...", end='')
        
        try:
            # K-means èšé¡
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(distance_matrix)
            
            # è¨ˆç®— Silhouette Score
            silhouette_avg = silhouette_score(
                distance_matrix, 
                labels, 
                metric='precomputed'
            )
            
            # è¨ˆç®—æ¯å€‹æ¨£æœ¬çš„ Silhouette Score
            silhouette_vals = silhouette_samples(
                distance_matrix,
                labels,
                metric='precomputed'
            )
            
            # è¨ˆç®—æ¯å€‹ cluster çš„çµ±è¨ˆ
            cluster_stats = {}
            for cluster_id in range(k):
                cluster_mask = labels == cluster_id
                cluster_scores = silhouette_vals[cluster_mask]
                cluster_stats[cluster_id] = {
                    'size': int(np.sum(cluster_mask)),
                    'mean': float(np.mean(cluster_scores)),
                    'std': float(np.std(cluster_scores)),
                    'min': float(np.min(cluster_scores)),
                    'max': float(np.max(cluster_scores))
                }
            
            # è¨ˆç®— cluster å¤§å°çš„æ¨™æº–å·®ï¼ˆå¹³è¡¡åº¦æŒ‡æ¨™ï¼‰
            cluster_sizes = [stats['size'] for stats in cluster_stats.values()]
            size_std = np.std(cluster_sizes)
            size_cv = size_std / np.mean(cluster_sizes) if np.mean(cluster_sizes) > 0 else 0
            
            results.append({
                'k': k,
                'silhouette_score': silhouette_avg,
                'silhouette_std': float(np.std(silhouette_vals)),
                'cluster_stats': cluster_stats,
                'cluster_size_std': size_std,
                'cluster_size_cv': size_cv,
                'min_cluster_size': min(cluster_sizes),
                'max_cluster_size': max(cluster_sizes)
            })
            
            print(f" Silhouette: {silhouette_avg:.4f}")
            
        except Exception as e:
            print(f" âœ— å¤±æ•—: {e}")
            continue
    
    return results


def find_optimal_k(results, method='silhouette'):
    """æ‰¾å‡ºæœ€ä½³ k å€¼"""
    if method == 'silhouette':
        # ä½¿ç”¨ Silhouette Score
        best_result = max(results, key=lambda x: x['silhouette_score'])
        return best_result
    elif method == 'elbow':
        # ä½¿ç”¨ Elbow æ–¹æ³•ï¼ˆéœ€è¦é¡å¤–è¨ˆç®— inertiaï¼‰
        pass
    
    return None


def plot_k_analysis(results, output_dir, generation):
    """ç¹ªè£½ k å€¼åˆ†æåœ–è¡¨"""
    print(f"\nğŸ¨ ç¹ªè£½åˆ†æåœ–è¡¨...")
    
    # æå–æ•¸æ“š
    k_values = [r['k'] for r in results]
    silhouette_scores = [r['silhouette_score'] for r in results]
    silhouette_stds = [r['silhouette_std'] for r in results]
    size_cvs = [r['cluster_size_cv'] for r in results]
    
    # å‰µå»ºåœ–è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Silhouette Score vs k
    ax = axes[0, 0]
    ax.plot(k_values, silhouette_scores, marker='o', linewidth=2, markersize=6)
    ax.set_xlabel('Number of Clusters (k)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Silhouette Score', fontsize=12, fontweight='bold')
    ax.set_title('Silhouette Score vs Number of Clusters', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # æ¨™è¨˜æœ€ä½³ k
    best_idx = np.argmax(silhouette_scores)
    best_k = k_values[best_idx]
    best_score = silhouette_scores[best_idx]
    ax.scatter([best_k], [best_score], color='red', s=200, marker='*', 
              zorder=5, label=f'Best k={best_k} (Score={best_score:.4f})')
    ax.legend(fontsize=11)
    
    # 2. Silhouette Score åˆ†å¸ƒï¼ˆå¸¶èª¤å·®æ¢ï¼‰
    ax = axes[0, 1]
    ax.errorbar(k_values, silhouette_scores, yerr=silhouette_stds, 
               fmt='o-', linewidth=2, markersize=6, capsize=5, alpha=0.7)
    ax.set_xlabel('Number of Clusters (k)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Silhouette Score (mean Â± std)', fontsize=12, fontweight='bold')
    ax.set_title('Silhouette Score Distribution', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 3. Cluster Size è®Šç•°ä¿‚æ•¸
    ax = axes[1, 0]
    ax.plot(k_values, size_cvs, marker='s', linewidth=2, markersize=6, color='orange')
    ax.set_xlabel('Number of Clusters (k)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Cluster Size CV', fontsize=12, fontweight='bold')
    ax.set_title('Cluster Size Variability (lower is better)', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 4. Top 10 k å€¼çš„è©³ç´°æ¯”è¼ƒ
    ax = axes[1, 1]
    top_10_indices = np.argsort(silhouette_scores)[-10:]
    top_10_k = [k_values[i] for i in top_10_indices]
    top_10_scores = [silhouette_scores[i] for i in top_10_indices]
    
    bars = ax.barh(range(len(top_10_k)), top_10_scores, color='skyblue', edgecolor='black')
    ax.set_yticks(range(len(top_10_k)))
    ax.set_yticklabels([f'k={k}' for k in top_10_k])
    ax.set_xlabel('Silhouette Score', fontsize=12, fontweight='bold')
    ax.set_title('Top 10 k Values by Silhouette Score', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='x')
    
    # æ¨™è¨˜æœ€ä½³
    best_in_top10 = top_10_scores.index(max(top_10_scores))
    bars[best_in_top10].set_color('gold')
    bars[best_in_top10].set_edgecolor('red')
    bars[best_in_top10].set_linewidth(2)
    
    plt.tight_layout()
    output_path = output_dir / f"generation_{generation:03d}_optimal_k_analysis.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"   âœ“ å·²å„²å­˜: {output_path.name}")
    plt.close()
    
    # ç¹ªè£½æ¯å€‹ k çš„ cluster å¤§å°åˆ†å¸ƒ
    plot_cluster_size_distribution(results, output_dir, generation)


def plot_cluster_size_distribution(results, output_dir, generation):
    """ç¹ªè£½ä¸åŒ k å€¼çš„ cluster å¤§å°åˆ†å¸ƒ"""
    print(f"   ç¹ªè£½ cluster å¤§å°åˆ†å¸ƒ...")
    
    # é¸æ“‡å¹¾å€‹ä»£è¡¨æ€§çš„ k å€¼
    k_values = [r['k'] for r in results]
    silhouette_scores = [r['silhouette_score'] for r in results]
    
    # é¸æ“‡æœ€ä½³çš„å’Œå¹¾å€‹ä»£è¡¨æ€§çš„ k
    best_idx = np.argmax(silhouette_scores)
    representative_indices = [
        0,  # æœ€å° k
        len(results) // 4,  # 1/4
        len(results) // 2,  # ä¸­é–“
        3 * len(results) // 4,  # 3/4
        best_idx,  # æœ€ä½³
        len(results) - 1  # æœ€å¤§ k
    ]
    representative_indices = sorted(set(representative_indices))
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()
    
    for idx, result_idx in enumerate(representative_indices[:6]):
        result = results[result_idx]
        k = result['k']
        cluster_stats = result['cluster_stats']
        
        ax = axes[idx]
        
        # æå– cluster å¤§å°
        cluster_ids = sorted(cluster_stats.keys())
        cluster_sizes = [cluster_stats[cid]['size'] for cid in cluster_ids]
        cluster_means = [cluster_stats[cid]['mean'] for cid in cluster_ids]
        
        # ç¹ªè£½æŸ±ç‹€åœ–
        bars = ax.bar(cluster_ids, cluster_sizes, color='skyblue', edgecolor='black', alpha=0.7)
        
        # æ ¹æ“š silhouette score è‘—è‰²
        for i, (bar, mean_score) in enumerate(zip(bars, cluster_means)):
            if mean_score > 0.5:
                bar.set_color('green')
            elif mean_score > 0.3:
                bar.set_color('yellow')
            else:
                bar.set_color('red')
        
        ax.set_xlabel('Cluster ID', fontsize=10)
        ax.set_ylabel('Cluster Size', fontsize=10)
        
        is_best = (result_idx == best_idx)
        title = f"k={k} (Silhouette={result['silhouette_score']:.4f})"
        if is_best:
            title += " â­ BEST"
            ax.set_title(title, fontsize=11, fontweight='bold', color='red')
        else:
            ax.set_title(title, fontsize=11)
        
        ax.grid(True, alpha=0.3, axis='y')
    
    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(len(representative_indices), 6):
        axes[idx].axis('off')
    
    plt.tight_layout()
    output_path = output_dir / f"generation_{generation:03d}_cluster_size_distribution.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"   âœ“ å·²å„²å­˜: {output_path.name}")
    plt.close()


def save_results_to_csv(results, output_dir, generation):
    """å„²å­˜çµæœåˆ° CSV"""
    print(f"\nğŸ’¾ å„²å­˜çµæœåˆ° CSV...")
    
    # åŸºæœ¬çµ±è¨ˆ
    basic_stats = []
    for r in results:
        basic_stats.append({
            'k': r['k'],
            'silhouette_score': r['silhouette_score'],
            'silhouette_std': r['silhouette_std'],
            'cluster_size_std': r['cluster_size_std'],
            'cluster_size_cv': r['cluster_size_cv'],
            'min_cluster_size': r['min_cluster_size'],
            'max_cluster_size': r['max_cluster_size']
        })
    
    df = pd.DataFrame(basic_stats)
    output_path = output_dir / f"generation_{generation:03d}_k_analysis.csv"
    df.to_csv(output_path, index=False)
    print(f"   âœ“ å·²å„²å­˜: {output_path.name}")
    
    # è©³ç´°çš„ cluster çµ±è¨ˆ
    detailed_stats = []
    for r in results:
        k = r['k']
        for cluster_id, stats in r['cluster_stats'].items():
            detailed_stats.append({
                'k': k,
                'cluster_id': cluster_id,
                'size': stats['size'],
                'silhouette_mean': stats['mean'],
                'silhouette_std': stats['std'],
                'silhouette_min': stats['min'],
                'silhouette_max': stats['max']
            })
    
    df_detailed = pd.DataFrame(detailed_stats)
    output_path_detailed = output_dir / f"generation_{generation:03d}_cluster_details.csv"
    df_detailed.to_csv(output_path_detailed, index=False)
    print(f"   âœ“ å·²å„²å­˜: {output_path_detailed.name}")


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python find_optimal_k.py <generation.pkl è·¯å¾‘> [k_min] [k_max]")
        print("ç¯„ä¾‹: python find_optimal_k.py portfolio_experiment_results/.../generations/generation_006_final.pkl 2 50")
        sys.exit(1)
    
    pkl_path = Path(sys.argv[1])
    k_min = int(sys.argv[2]) if len(sys.argv) > 2 else 2
    k_max = int(sys.argv[3]) if len(sys.argv) > 3 else 50
    
    if not pkl_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {pkl_path}")
        sys.exit(1)
    
    print("\n" + "="*100)
    print("ğŸ” å°‹æ‰¾æœ€ä½³ Cluster æ•¸é‡ (k)")
    print("="*100 + "\n")
    
    # 1. è¼‰å…¥ generation
    data = load_generation(pkl_path)
    population = data['population']
    generation = data['generation']
    
    # 2. è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
    similarity_matrix = compute_similarity_matrix(population, use_parallel=True)
    
    # 3. è©•ä¼°ä¸åŒ k å€¼
    results = evaluate_k_range(similarity_matrix, k_min, k_max)
    
    if not results:
        print("âŒ æ²’æœ‰æˆåŠŸè©•ä¼°ä»»ä½• k å€¼")
        sys.exit(1)
    
    # 4. æ‰¾å‡ºæœ€ä½³ k
    print(f"\nğŸ† åˆ†æçµæœ:")
    best_result = find_optimal_k(results, method='silhouette')
    
    print(f"\n{'='*100}")
    print(f"â­ æœ€ä½³ k å€¼: {best_result['k']}")
    print(f"{'='*100}")
    print(f"  Silhouette Score: {best_result['silhouette_score']:.4f}")
    print(f"  Silhouette Std: {best_result['silhouette_std']:.4f}")
    print(f"  Cluster Size CV: {best_result['cluster_size_cv']:.4f}")
    print(f"  Cluster Size Range: [{best_result['min_cluster_size']}, {best_result['max_cluster_size']}]")
    
    print(f"\n  å„ Cluster è©³ç´°ä¿¡æ¯:")
    for cluster_id, stats in best_result['cluster_stats'].items():
        print(f"    Cluster {cluster_id}: size={stats['size']}, "
              f"silhouette={stats['mean']:.4f} (Â±{stats['std']:.4f})")
    
    # é¡¯ç¤º Top 5
    print(f"\nğŸ“Š Top 5 k å€¼:")
    sorted_results = sorted(results, key=lambda x: x['silhouette_score'], reverse=True)
    for i, r in enumerate(sorted_results[:5], 1):
        print(f"  {i}. k={r['k']}: Silhouette={r['silhouette_score']:.4f}, "
              f"Size CV={r['cluster_size_cv']:.4f}")
    
    # 5. å‰µå»ºè¼¸å‡ºç›®éŒ„
    exp_dir = pkl_path.parent.parent
    output_dir = exp_dir / "optimal_k_analysis"
    output_dir.mkdir(exist_ok=True)
    print(f"\nğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # 6. ç¹ªè£½åœ–è¡¨
    plot_k_analysis(results, output_dir, generation)
    
    # 7. å„²å­˜çµæœ
    save_results_to_csv(results, output_dir, generation)
    
    # 8. å®Œæˆ
    print("\n" + "="*100)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*100)
    print(f"\nğŸ“Š ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - generation_{generation:03d}_optimal_k_analysis.png")
    print(f"  - generation_{generation:03d}_cluster_size_distribution.png")
    print(f"  - generation_{generation:03d}_k_analysis.csv")
    print(f"  - generation_{generation:03d}_cluster_details.csv")
    print(f"\nğŸ“ ä¿å­˜ä½ç½®: {output_dir}")
    print()


if __name__ == "__main__":
    main()
