#!/usr/bin/env python3
"""
è¨ˆç®—å¯¦é©—ä¸­æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™

é€™å€‹è…³æœ¬æœƒï¼š
1. è¼‰å…¥æ‰€æœ‰ generation_*.pkl æ–‡ä»¶
2. ä½¿ç”¨ä¸¦è¡Œè¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
3. è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™ï¼ˆå¹³å‡ç›¸ä¼¼åº¦ã€å¤šæ¨£æ€§åˆ†æ•¸ç­‰ï¼‰
4. å„²å­˜çµæœåˆ° diversity_metrics.json

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/analysis/compute_diversity_metrics.py \
        --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353 \
        --n_workers 8
"""

import argparse
import json
import pickle
import time
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
import numpy as np
from tqdm import tqdm

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from gp_quant.similarity import ParallelSimilarityMatrix, SimilarityMatrix


def compute_single_generation(pkl_file: Path, use_parallel: bool = True, n_workers: int = 8):
    """
    è¨ˆç®—å–®ä¸€ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™
    
    Args:
        pkl_file: generation_XXX.pkl æ–‡ä»¶è·¯å¾‘
        use_parallel: æ˜¯å¦ä½¿ç”¨ä¸¦è¡Œè¨ˆç®—
        n_workers: ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸
        
    Returns:
        dict: å¤šæ¨£æ€§æŒ‡æ¨™
    """
    try:
        # è¼‰å…¥æ—ç¾¤
        with open(pkl_file, 'rb') as f:
            population = pickle.load(f)
        
        # æå–ä¸–ä»£ç·¨è™Ÿ
        gen_num = int(pkl_file.stem.split('_')[1])
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        start_time = time.time()
        
        if use_parallel and len(population) >= 200:
            sim_matrix = ParallelSimilarityMatrix(population, n_workers=n_workers)
        else:
            sim_matrix = SimilarityMatrix(population)
        
        similarity_matrix = sim_matrix.compute(show_progress=False)
        
        computation_time = time.time() - start_time
        
        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        # æ’é™¤å°è§’ç·šï¼ˆè‡ªå·±èˆ‡è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        similarities = similarity_matrix[mask]
        
        metrics = {
            'generation': gen_num,
            'population_size': len(population),
            'avg_similarity': float(np.mean(similarities)),
            'diversity_score': float(1.0 - np.mean(similarities)),
            'std_similarity': float(np.std(similarities)),
            'min_similarity': float(np.min(similarities)),
            'max_similarity': float(np.max(similarities)),
            'median_similarity': float(np.median(similarities)),
            'computation_time': computation_time
        }
        
        return metrics
        
    except Exception as e:
        print(f"âœ— è™•ç† {pkl_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None


def compute_all_diversity_metrics(exp_dir: Path, n_workers: int = 8, batch_parallel: bool = True):
    """
    è¨ˆç®—æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™
    
    Args:
        exp_dir: å¯¦é©—ç›®éŒ„
        n_workers: ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸
        batch_parallel: æ˜¯å¦ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œï¼ˆåŒæ™‚è™•ç†å¤šå€‹ä¸–ä»£ï¼‰
        
    Returns:
        dict: å®Œæ•´çš„å¤šæ¨£æ€§æŒ‡æ¨™æ•¸æ“š
    """
    generations_dir = exp_dir / 'generations'
    
    if not generations_dir.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° generations ç›®éŒ„: {generations_dir}")
    
    # ç²å–æ‰€æœ‰ pkl æ–‡ä»¶ä¸¦æ’åº
    pkl_files = sorted(generations_dir.glob('generation_*.pkl'))
    
    if not pkl_files:
        raise FileNotFoundError(f"åœ¨ {generations_dir} ä¸­æ‰¾ä¸åˆ° generation_*.pkl æ–‡ä»¶")
    
    print(f"æ‰¾åˆ° {len(pkl_files)} å€‹ä¸–ä»£æ–‡ä»¶")
    print()
    
    # è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™
    all_metrics = []
    
    if batch_parallel:
        # æ‰¹æ¬¡ä¸¦è¡Œï¼šåŒæ™‚è™•ç†å¤šå€‹ä¸–ä»£
        print(f"ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œè¨ˆç®—ï¼ˆ{n_workers} workersï¼‰...")
        print()
        
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_file = {
                executor.submit(compute_single_generation, pkl_file, True, 8): pkl_file
                for pkl_file in pkl_files
            }
            
            # æ”¶é›†çµæœï¼ˆä½¿ç”¨é€²åº¦æ¢ï¼‰
            with tqdm(total=len(pkl_files), desc="è¨ˆç®—å¤šæ¨£æ€§") as pbar:
                for future in as_completed(future_to_file):
                    pkl_file = future_to_file[future]
                    try:
                        metrics = future.result()
                        if metrics:
                            all_metrics.append(metrics)
                        pbar.update(1)
                    except Exception as e:
                        print(f"âœ— {pkl_file.name} è¨ˆç®—å¤±æ•—: {e}")
                        pbar.update(1)
    else:
        # åºåˆ—è¨ˆç®—ï¼šä¸€å€‹ä¸€å€‹è™•ç†
        print("ä½¿ç”¨åºåˆ—è¨ˆç®—...")
        print()
        
        for pkl_file in tqdm(pkl_files, desc="è¨ˆç®—å¤šæ¨£æ€§"):
            metrics = compute_single_generation(pkl_file, True, n_workers)
            if metrics:
                all_metrics.append(metrics)
    
    # æŒ‰ä¸–ä»£ç·¨è™Ÿæ’åº
    all_metrics.sort(key=lambda x: x['generation'])
    
    # æ§‹å»ºå®Œæ•´æ•¸æ“š
    result = {
        'experiment': exp_dir.name,
        'experiment_path': str(exp_dir),
        'total_generations': len(all_metrics),
        'population_size': all_metrics[0]['population_size'] if all_metrics else 0,
        'computation_date': datetime.now().isoformat(),
        'n_workers': n_workers,
        'batch_parallel': batch_parallel,
        'total_computation_time': sum(m['computation_time'] for m in all_metrics),
        'metrics': all_metrics
    }
    
    return result


def main():
    parser = argparse.ArgumentParser(
        description='è¨ˆç®—å¯¦é©—ä¸­æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python scripts/analysis/compute_diversity_metrics.py \\
      --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353
  
  # æŒ‡å®šä¸¦è¡Œæ•¸
  python scripts/analysis/compute_diversity_metrics.py \\
      --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353 \\
      --n_workers 8
  
  # ä½¿ç”¨åºåˆ—è¨ˆç®—ï¼ˆä¸ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œï¼‰
  python scripts/analysis/compute_diversity_metrics.py \\
      --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353 \\
      --no_batch_parallel
        """
    )
    
    parser.add_argument(
        '--exp_dir',
        type=str,
        required=True,
        help='å¯¦é©—ç›®éŒ„è·¯å¾‘'
    )
    
    parser.add_argument(
        '--n_workers',
        type=int,
        default=8,
        help='ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸ï¼ˆé è¨­: 8ï¼‰'
    )
    
    parser.add_argument(
        '--no_batch_parallel',
        action='store_true',
        help='ä¸ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œï¼ˆä¸€æ¬¡åªè™•ç†ä¸€å€‹ä¸–ä»£ï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆé è¨­: exp_dir/diversity_metrics.jsonï¼‰'
    )
    
    args = parser.parse_args()
    
    # è§£æè·¯å¾‘
    exp_dir = Path(args.exp_dir)
    
    if not exp_dir.exists():
        print(f"âœ— å¯¦é©—ç›®éŒ„ä¸å­˜åœ¨: {exp_dir}")
        return 1
    
    # è¨­ç½®è¼¸å‡ºè·¯å¾‘
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = exp_dir / 'diversity_metrics.json'
    
    print("="*80)
    print("ğŸ“Š è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™")
    print("="*80)
    print()
    print(f"å¯¦é©—ç›®éŒ„: {exp_dir}")
    print(f"ä¸¦è¡Œæ•¸: {args.n_workers}")
    print(f"æ‰¹æ¬¡ä¸¦è¡Œ: {'æ˜¯' if not args.no_batch_parallel else 'å¦'}")
    print(f"è¼¸å‡ºæ–‡ä»¶: {output_file}")
    print()
    
    # è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™
    try:
        start_time = time.time()
        
        result = compute_all_diversity_metrics(
            exp_dir,
            n_workers=args.n_workers,
            batch_parallel=not args.no_batch_parallel
        )
        
        total_time = time.time() - start_time
        
        print()
        print("="*80)
        print("âœ… è¨ˆç®—å®Œæˆ")
        print("="*80)
        print()
        print(f"ç¸½ä¸–ä»£æ•¸: {result['total_generations']}")
        print(f"æ—ç¾¤å¤§å°: {result['population_size']}")
        print(f"ç¸½è¨ˆç®—æ™‚é–“: {total_time:.1f}s ({total_time/60:.1f} åˆ†é˜)")
        print(f"å¹³å‡æ¯ä»£: {total_time/result['total_generations']:.1f}s")
        print()
        
        # é¡¯ç¤ºå¤šæ¨£æ€§è¶¨å‹¢
        print("ğŸ“ˆ å¤šæ¨£æ€§è¶¨å‹¢:")
        metrics = result['metrics']
        first_gen = metrics[0]
        last_gen = metrics[-1]
        
        print(f"  ç¬¬ {first_gen['generation']} ä»£:")
        print(f"    å¹³å‡ç›¸ä¼¼åº¦: {first_gen['avg_similarity']:.4f}")
        print(f"    å¤šæ¨£æ€§åˆ†æ•¸: {first_gen['diversity_score']:.4f}")
        print()
        print(f"  ç¬¬ {last_gen['generation']} ä»£:")
        print(f"    å¹³å‡ç›¸ä¼¼åº¦: {last_gen['avg_similarity']:.4f}")
        print(f"    å¤šæ¨£æ€§åˆ†æ•¸: {last_gen['diversity_score']:.4f}")
        print()
        
        diversity_change = last_gen['diversity_score'] - first_gen['diversity_score']
        print(f"  å¤šæ¨£æ€§è®ŠåŒ–: {diversity_change:+.4f} ({diversity_change/first_gen['diversity_score']*100:+.1f}%)")
        print()
        
        # å„²å­˜çµæœ
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"âœ“ çµæœå·²å„²å­˜: {output_file}")
        print()
        
        return 0
        
    except Exception as e:
        print()
        print(f"âœ— è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
