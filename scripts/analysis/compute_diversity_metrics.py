#!/usr/bin/env python3
"""
è¨ˆç®—å¯¦é©—ä¸­æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™

é€™å€‹è…³æœ¬æœƒï¼š
1. è¼‰å…¥æ‰€æœ‰ generation_*.pkl æ–‡ä»¶
2. ä½¿ç”¨ä¸¦è¡Œè¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
3. è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™ï¼ˆå¹³å‡ç›¸ä¼¼åº¦ã€å¤šæ¨£æ€§åˆ†æ•¸ç­‰ï¼‰
4. å„²å­˜çµæœåˆ° diversity_metrics.json

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/analysis/compute_diversity_metrics.py \
        --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353 \
        --n_workers 8
"""

import argparse
import json
import pickle
import time
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime
import numpy as np
from tqdm import tqdm

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

print("ğŸ”§ æ­£åœ¨è¼‰å…¥æ¨¡çµ„...", flush=True)
from gp_quant.similarity import ParallelSimilarityMatrix, SimilarityMatrix

# åˆå§‹åŒ– DEAP creatorï¼ˆç”¨æ–¼ pickle ååºåˆ—åŒ–ï¼‰
from deap import base, creator, gp, tools
import operator

print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– DEAP...", flush=True)
# è¨­ç½® DEAP creatorï¼ˆå¦‚æœå°šæœªè¨­ç½®ï¼‰
if not hasattr(creator, "FitnessMax"):
    creator.create("FitnessMax", base.Fitness, weights=(1.0,))
if not hasattr(creator, "Individual"):
    creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)
print("âœ… åˆå§‹åŒ–å®Œæˆ", flush=True)


def compute_single_generation(pkl_file: Path, use_parallel: bool = True, n_workers: int = 8):
    """
    è¨ˆç®—å–®ä¸€ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™
    
    Args:
        pkl_file: generation_XXX.pkl æ–‡ä»¶è·¯å¾‘
        use_parallel: æ˜¯å¦ä½¿ç”¨ä¸¦è¡Œè¨ˆç®—
        n_workers: ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸
        
    Returns:
        dict: å¤šæ¨£æ€§æŒ‡æ¨™
    """
    try:
        # è¼‰å…¥æ—ç¾¤
        load_start = time.time()
        with open(pkl_file, 'rb') as f:
            data = pickle.load(f)
        
        # æå–æ—ç¾¤ï¼ˆè™•ç†å­—å…¸æ ¼å¼ï¼‰
        if isinstance(data, dict) and 'population' in data:
            population = data['population']
        else:
            population = data
        
        # æå–ä¸–ä»£ç·¨è™Ÿ
        gen_num = int(pkl_file.stem.split('_')[1])
        
        # è¨ˆç®—æ¨¹çš„çµ±è¨ˆè³‡è¨Š
        tree_sizes = [len(ind) for ind in population]
        tree_depths = [ind.height for ind in population]
        avg_size = sum(tree_sizes) / len(tree_sizes)
        max_size = max(tree_sizes)
        avg_depth = sum(tree_depths) / len(tree_depths)
        max_depth = max(tree_depths)
        
        print(f"\n{'='*70}", flush=True)
        print(f"ğŸ”„ è™•ç† Generation {gen_num}", flush=True)
        print(f"{'='*70}", flush=True)
        print(f"  æ—ç¾¤å¤§å°: {len(population)}", flush=True)
        print(f"  å¹³å‡æ¨¹æ·±åº¦: {avg_depth:.1f} (æœ€å¤§: {max_depth})", flush=True)
        print(f"  å¹³å‡ç¯€é»æ•¸: {avg_size:.1f} (æœ€å¤§: {max_size})", flush=True)
        print(f"  è¼‰å…¥æ™‚é–“: {time.time() - load_start:.1f} ç§’", flush=True)
        print(f"  é–‹å§‹è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£...", flush=True)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        start_time = time.time()
        
        if use_parallel and len(population) >= 200:
            sim_matrix = ParallelSimilarityMatrix(population, n_workers=n_workers)
        else:
            sim_matrix = SimilarityMatrix(population)
        
        similarity_matrix = sim_matrix.compute(show_progress=False)
        
        computation_time = time.time() - start_time
        
        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        # æ’é™¤å°è§’ç·šï¼ˆè‡ªå·±èˆ‡è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        mask = ~np.eye(similarity_matrix.shape[0], dtype=bool)
        similarities = similarity_matrix[mask]
        
        avg_similarity = float(np.mean(similarities))
        diversity_score = float(1.0 - np.mean(similarities))
        
        metrics = {
            'generation': gen_num,
            'population_size': len(population),
            'avg_similarity': avg_similarity,
            'diversity_score': diversity_score,
            'std_similarity': float(np.std(similarities)),
            'min_similarity': float(np.min(similarities)),
            'max_similarity': float(np.max(similarities)),
            'median_similarity': float(np.median(similarities)),
            'computation_time': computation_time
        }
        
        # è¼¸å‡ºå®Œæˆè³‡è¨Š
        print(f"  âœ… è¨ˆç®—å®Œæˆï¼", flush=True)
        print(f"  è¨ˆç®—æ™‚é–“: {computation_time:.1f} ç§’ ({computation_time/60:.2f} åˆ†é˜)", flush=True)
        print(f"  å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}", flush=True)
        print(f"  å¤šæ¨£æ€§åˆ†æ•¸: {diversity_score:.4f}", flush=True)
        print(f"{'='*70}\n", flush=True)
        
        return metrics
        
    except Exception as e:
        print(f"âœ— è™•ç† {pkl_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None


def compute_all_generations(exp_dir: Path, n_workers: int = 8, batch_parallel: bool = True, cooldown: float = 0) -> dict:
    """
    è¨ˆç®—æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™
    
    Args:
        exp_dir: å¯¦é©—ç›®éŒ„
        n_workers: ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸
        batch_parallel: æ˜¯å¦ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œï¼ˆåŒæ™‚è™•ç†å¤šå€‹ä¸–ä»£ï¼‰
        cooldown: æ¯å€‹ä¸–ä»£è¨ˆç®—å¾Œçš„å†·å»æ™‚é–“ï¼ˆç§’ï¼‰
        
    Returns:
        dict: å®Œæ•´çš„å¤šæ¨£æ€§æŒ‡æ¨™æ•¸æ“š
    """
    generations_dir = exp_dir / 'generations'
    
    if not generations_dir.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° generations ç›®éŒ„: {generations_dir}")
    
    # ç²å–æ‰€æœ‰ pkl æ–‡ä»¶ä¸¦æ’åº
    pkl_files = sorted(generations_dir.glob('generation_*.pkl'))
    
    if not pkl_files:
        raise FileNotFoundError(f"åœ¨ {generations_dir} ä¸­æ‰¾ä¸åˆ° generation_*.pkl æ–‡ä»¶")
    
    print(f"æ‰¾åˆ° {len(pkl_files)} å€‹ä¸–ä»£æ–‡ä»¶")
    print()
    
    # è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™
    all_metrics = []
    
    if batch_parallel:
        # æ‰¹æ¬¡ä¸¦è¡Œï¼šåŒæ™‚è™•ç†å¤šå€‹ä¸–ä»£
        print(f"ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œè¨ˆç®—ï¼ˆ{n_workers} workersï¼‰...")
        print()
        
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_file = {
                executor.submit(compute_single_generation, pkl_file, True, 8): pkl_file
                for pkl_file in pkl_files
            }
            
            # æ”¶é›†çµæœï¼ˆä½¿ç”¨é€²åº¦æ¢ï¼‰
            with tqdm(total=len(pkl_files), desc="è¨ˆç®—å¤šæ¨£æ€§") as pbar:
                for future in as_completed(future_to_file):
                    pkl_file = future_to_file[future]
                    try:
                        metrics = future.result()
                        if metrics:
                            all_metrics.append(metrics)
                        pbar.update(1)
                    except Exception as e:
                        print(f"âœ— {pkl_file.name} è¨ˆç®—å¤±æ•—: {e}")
                        pbar.update(1)
    else:
        # åºåˆ—è¨ˆç®—ï¼šä¸€å€‹ä¸€å€‹è™•ç†
        print("ä½¿ç”¨åºåˆ—è¨ˆç®—...")
        print(f"ç¸½å…± {len(pkl_files)} å€‹ä¸–ä»£")
        print()
        
        for idx, pkl_file in enumerate(pkl_files, 1):
            print(f"\nğŸ“Š é€²åº¦: {idx}/{len(pkl_files)} ({idx*100//len(pkl_files)}%)", flush=True)
            metrics = compute_single_generation(pkl_file, True, n_workers)
            if metrics:
                all_metrics.append(metrics)
            
            # å†·å»æ™‚é–“
            if cooldown > 0:
                print(f"â¸ï¸  å†·å» {cooldown} ç§’...", flush=True)
                time.sleep(cooldown)
    
    # æŒ‰ä¸–ä»£ç·¨è™Ÿæ’åº
    all_metrics.sort(key=lambda x: x['generation'])
    
    # æ§‹å»ºå®Œæ•´æ•¸æ“š
    result = {
        'experiment': exp_dir.name,
        'experiment_path': str(exp_dir),
        'total_generations': len(all_metrics),
        'population_size': all_metrics[0]['population_size'] if all_metrics else 0,
        'computation_date': datetime.now().isoformat(),
        'n_workers': n_workers,
        'batch_parallel': batch_parallel,
        'total_computation_time': sum(m['computation_time'] for m in all_metrics),
        'metrics': all_metrics
    }
    
    return result


def main():
    parser = argparse.ArgumentParser(
        description='è¨ˆç®—å¯¦é©—ä¸­æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # åŸºæœ¬ä½¿ç”¨
  python scripts/analysis/compute_diversity_metrics.py \\
      --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353
  
  # æŒ‡å®šä¸¦è¡Œæ•¸
  python scripts/analysis/compute_diversity_metrics.py \\
      --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353 \\
      --n_workers 8
  
  # ä½¿ç”¨åºåˆ—è¨ˆç®—ï¼ˆä¸ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œï¼‰
  python scripts/analysis/compute_diversity_metrics.py \\
      --exp_dir portfolio_experiment_results/portfolio_exp_sharpe_20251014_191353 \\
      --no_batch_parallel
        """
    )
    
    parser.add_argument(
        '--exp_dir',
        type=str,
        required=True,
        help='å¯¦é©—ç›®éŒ„è·¯å¾‘'
    )
    
    parser.add_argument(
        '--n_workers',
        type=int,
        default=8,
        help='ä¸¦è¡Œå·¥ä½œé€²ç¨‹æ•¸ï¼ˆé è¨­: 8ï¼‰'
    )
    
    parser.add_argument(
        '--no_batch_parallel',
        action='store_true',
        help='ä¸ä½¿ç”¨æ‰¹æ¬¡ä¸¦è¡Œï¼ˆä¸€æ¬¡åªè™•ç†ä¸€å€‹ä¸–ä»£ï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ï¼ˆé è¨­: exp_dir/diversity_metrics.jsonï¼‰'
    )
    
    parser.add_argument(
        '--cooldown',
        type=float,
        default=0,
        help='æ¯å€‹ä¸–ä»£è¨ˆç®—å¾Œçš„å†·å»æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œç”¨æ–¼é™ä½ CPU æº«åº¦ï¼ˆé è¨­: 0ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è§£æè·¯å¾‘
    exp_dir = Path(args.exp_dir)
    
    if not exp_dir.exists():
        print(f"âœ— å¯¦é©—ç›®éŒ„ä¸å­˜åœ¨: {exp_dir}")
        return 1
    
    # è¨­ç½®è¼¸å‡ºè·¯å¾‘
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = exp_dir / 'diversity_metrics.json'
    
    print("="*80)
    print("ğŸ“Š è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™")
    print("="*80)
    print()
    print(f"å¯¦é©—ç›®éŒ„: {exp_dir}")
    print(f"ä¸¦è¡Œæ•¸: {args.n_workers}")
    print(f"æ‰¹æ¬¡ä¸¦è¡Œ: {'æ˜¯' if not args.no_batch_parallel else 'å¦'}")
    if args.cooldown > 0:
        print(f"å†·å»æ™‚é–“: {args.cooldown} ç§’/ä¸–ä»£")
    print(f"è¼¸å‡ºæ–‡ä»¶: {output_file}")
    print()
    
    # è¨ˆç®—å¤šæ¨£æ€§æŒ‡æ¨™
    try:
        start_time = time.time()
        
        result = compute_all_generations(
            exp_dir,
            n_workers=args.n_workers,
            batch_parallel=not args.no_batch_parallel,
            cooldown=args.cooldown
        )
        
        total_time = time.time() - start_time
        
        print()
        print("="*80)
        print("âœ… è¨ˆç®—å®Œæˆ")
        print("="*80)
        print()
        print(f"ç¸½ä¸–ä»£æ•¸: {result['total_generations']}")
        print(f"æ—ç¾¤å¤§å°: {result['population_size']}")
        print(f"ç¸½è¨ˆç®—æ™‚é–“: {total_time:.1f}s ({total_time/60:.1f} åˆ†é˜)")
        print(f"å¹³å‡æ¯ä»£: {total_time/result['total_generations']:.1f}s")
        print()
        
        # é¡¯ç¤ºå¤šæ¨£æ€§è¶¨å‹¢
        print("ğŸ“ˆ å¤šæ¨£æ€§è¶¨å‹¢:")
        metrics = result['metrics']
        first_gen = metrics[0]
        last_gen = metrics[-1]
        
        print(f"  ç¬¬ {first_gen['generation']} ä»£:")
        print(f"    å¹³å‡ç›¸ä¼¼åº¦: {first_gen['avg_similarity']:.4f}")
        print(f"    å¤šæ¨£æ€§åˆ†æ•¸: {first_gen['diversity_score']:.4f}")
        print()
        print(f"  ç¬¬ {last_gen['generation']} ä»£:")
        print(f"    å¹³å‡ç›¸ä¼¼åº¦: {last_gen['avg_similarity']:.4f}")
        print(f"    å¤šæ¨£æ€§åˆ†æ•¸: {last_gen['diversity_score']:.4f}")
        print()
        
        diversity_change = last_gen['diversity_score'] - first_gen['diversity_score']
        if first_gen['diversity_score'] != 0:
            change_pct = (diversity_change / first_gen['diversity_score']) * 100
            print(f"  å¤šæ¨£æ€§è®ŠåŒ–: {diversity_change:+.4f} ({change_pct:+.1f}%)")
        else:
            print(f"  å¤šæ¨£æ€§è®ŠåŒ–: {diversity_change:+.4f}")
        print()
        
        # å„²å­˜çµæœ
        with open(output_file, 'w') as f:
            json.dump(result, f, indent=2)
        
        print(f"âœ“ çµæœå·²å„²å­˜: {output_file}")
        print()
        
        return 0
        
    except Exception as e:
        print()
        print(f"âœ— è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
