#!/usr/bin/env python3
"""
Visualize Diversity Evolution

è¨ˆç®—æ‰€æœ‰ä¸–ä»£çš„å¤šæ¨£æ€§æŒ‡æ¨™ï¼ˆå¹³å‡ PnL correlation å’Œå¹³å‡æ¨™æº–åŒ– TED distanceï¼‰ï¼Œ
ä¸¦ç¹ªè£½æ¼”åŒ–è¶¨å‹¢æŠ˜ç·šåœ–ã€‚
"""

import sys
import pickle
import json
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Any, Tuple
from deap import creator, base, gp
from joblib import Parallel, delayed
from tqdm import tqdm

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from gp_quant.evolution.components.gp import operators
from gp_quant.evolution.components.backtesting import PortfolioBacktestingEngine
from gp_quant.data.loader import load_and_process_data, split_train_test_data
from gp_quant.similarity.tree_edit_distance import compute_ted


def setup_deap_creator():
    """è¨­ç½® DEAP creator"""
    if not hasattr(creator, "FitnessMax"):
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
    if not hasattr(creator, "Individual"):
        creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)


def load_all_generations(records_dir: Path) -> Dict[int, List]:
    """
    è¼‰å…¥æ‰€æœ‰ä¸–ä»£çš„æ—ç¾¤
    
    Args:
        records_dir: è¨˜éŒ„ç›®éŒ„è·¯å¾‘
        
    Returns:
        å­—å…¸ï¼Œéµç‚ºä¸–ä»£è™Ÿï¼Œå€¼ç‚ºæ—ç¾¤åˆ—è¡¨
    """
    populations = {}
    populations_dir = records_dir / 'populations'
    
    if not populations_dir.exists():
        raise ValueError(f"Populations directory not found: {populations_dir}")
    
    gen_files = sorted(populations_dir.glob('generation_*.pkl'))
    
    if len(gen_files) == 0:
        raise ValueError("No generation files found")
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(gen_files)} å€‹ä¸–ä»£æ–‡ä»¶")
    
    for gen_file in gen_files:
        gen_num = int(gen_file.stem.split('_')[1])
        
        try:
            with open(gen_file, 'rb') as f:
                population = pickle.load(f)
            populations[gen_num] = population
        except Exception as e:
            print(f"   âš ï¸  è¼‰å…¥ä¸–ä»£ {gen_num} å¤±æ•—: {e}")
    
    print(f"âœ… æˆåŠŸè¼‰å…¥ {len(populations)} å€‹ä¸–ä»£\n")
    
    return populations


def sample_population(population: List, sample_size: int, strategy: str = 'stratified') -> List:
    """
    å¾æ—ç¾¤ä¸­æ¡æ¨£
    
    Args:
        population: å®Œæ•´æ—ç¾¤
        sample_size: æ¡æ¨£å¤§å°
        strategy: æ¡æ¨£ç­–ç•¥
        
    Returns:
        æ¡æ¨£å¾Œçš„æ—ç¾¤
    """
    n = len(population)
    
    if sample_size >= n:
        return population
    
    if strategy == 'stratified':
        sorted_pop = sorted(population, key=lambda ind: ind.fitness.values[0], reverse=True)
        indices = np.linspace(0, n - 1, sample_size, dtype=int)
        sampled = [sorted_pop[i] for i in indices]
    else:
        indices = np.random.choice(n, sample_size, replace=False)
        sampled = [population[i] for i in indices]
    
    return sampled


def calculate_pnl_for_individual_worker(individual: Any, 
                                         train_data_dict: Dict,
                                         backtest_start: str,
                                         backtest_end: str) -> np.ndarray:
    """
    è¨ˆç®—å–®å€‹å€‹é«”çš„ PnL curveï¼ˆworker å‡½æ•¸ï¼Œåœ¨å­é€²ç¨‹ä¸­åŸ·è¡Œï¼‰
    
    æ¯å€‹å­é€²ç¨‹å‰µå»ºè‡ªå·±çš„ engineï¼Œé¿å…åºåˆ—åŒ–å•é¡Œ
    """
    try:
        # åœ¨å­é€²ç¨‹ä¸­å‰µå»º engine
        engine = PortfolioBacktestingEngine(
            data=train_data_dict,
            backtest_start=backtest_start,
            backtest_end=backtest_end,
            initial_capital=100000.0
        )
        pnl_curve = engine.get_pnl_curve(individual)
        return pnl_curve.values
    except Exception as e:
        # å¦‚æœå¤±æ•—ï¼Œè¿”å›å…¨é›¶ï¼ˆéœ€è¦çŸ¥é“é•·åº¦ï¼Œä½¿ç”¨å›ºå®šå€¼ï¼‰
        return np.zeros(504)  # é»˜èªè¨“ç·´æœŸé•·åº¦


def calculate_pnl_diversity(population: List,
                            train_data_dict: Dict,
                            backtest_start: str,
                            backtest_end: str,
                            n_jobs: int = 4) -> Tuple[float, int, int]:
    """
    è¨ˆç®— PnL diversityï¼ˆå¹³å‡ç›¸é—œæ€§ï¼‰
    
    Args:
        population: æ—ç¾¤åˆ—è¡¨
        train_data_dict: è¨“ç·´æ•¸æ“šå­—å…¸
        backtest_start: å›æ¸¬é–‹å§‹æ—¥æœŸ
        backtest_end: å›æ¸¬çµæŸæ—¥æœŸ
        n_jobs: å¹³è¡Œè™•ç†å™¨æ•¸é‡
        
    Returns:
        (mean_correlation, valid_count, total_count)
    """
    n = len(population)
    
    # å¹³è¡Œè¨ˆç®—æ‰€æœ‰å€‹é«”çš„ PnL curves
    # ä½¿ç”¨ threading backend é¿å… DEAP creator çš„ pickle å•é¡Œ
    pnl_curves = Parallel(n_jobs=n_jobs, backend='threading')(
        delayed(calculate_pnl_for_individual_worker)(
            ind, train_data_dict, backtest_start, backtest_end
        )
        for ind in population
    )
    
    # è½‰æ›ç‚ºçŸ©é™£
    pnl_matrix = np.array(pnl_curves)
    
    # æª¢æŸ¥æœ‰æ•ˆæ€§
    valid_mask = ~np.all(pnl_matrix == 0, axis=1)
    invalid_indices = np.where(~valid_mask)[0].tolist()
    valid_count = np.sum(valid_mask)
    
    # è¨ˆç®—ç›¸é—œæ€§çŸ©é™£
    corr_matrix = np.corrcoef(pnl_matrix)
    corr_matrix = np.nan_to_num(corr_matrix, nan=0.0)
    
    # å°æ–¼ç„¡æ•ˆå€‹é«”ï¼Œè¨­ç‚º 0
    for idx in invalid_indices:
        corr_matrix[idx, :] = 0.0
        corr_matrix[:, idx] = 0.0
        corr_matrix[idx, idx] = 1.0
    
    np.fill_diagonal(corr_matrix, 1.0)
    
    # è¨ˆç®—æœ‰æ•ˆå€‹é«”ä¹‹é–“çš„å¹³å‡ç›¸é—œæ€§
    valid_pairs = []
    for i in range(n):
        for j in range(i + 1, n):
            if i not in invalid_indices and j not in invalid_indices:
                valid_pairs.append(corr_matrix[i, j])
    
    if len(valid_pairs) > 0:
        mean_corr = np.mean(valid_pairs)
    else:
        mean_corr = 0.0
    
    return mean_corr, valid_count, n


def calculate_ted_for_pair(i: int, j: int, ind_i: Any, ind_j: Any) -> Tuple[int, int, float]:
    """è¨ˆç®—ä¸€å°å€‹é«”çš„æ¨™æº–åŒ– TED"""
    try:
        ted = compute_ted(ind_i, ind_j)
        max_size = max(len(ind_i), len(ind_j))
        norm_ted = ted / max_size if max_size > 0 else 0.0
        return i, j, norm_ted
    except Exception as e:
        return i, j, 1.0


def calculate_ted_diversity(population: List, n_jobs: int = 4) -> float:
    """
    è¨ˆç®— TED diversityï¼ˆå¹³å‡è·é›¢ï¼‰
    
    Args:
        population: æ—ç¾¤åˆ—è¡¨
        n_jobs: å¹³è¡Œè™•ç†å™¨æ•¸é‡
        
    Returns:
        mean_ted_distance
    """
    n = len(population)
    
    # åˆå§‹åŒ–çŸ©é™£
    ted_matrix = np.zeros((n, n))
    
    # ç”Ÿæˆæ‰€æœ‰éœ€è¦è¨ˆç®—çš„é…å°
    pairs = [(i, j, population[i], population[j]) 
             for i in range(n) for j in range(i + 1, n)]
    
    # å¹³è¡Œè¨ˆç®—
    # ä½¿ç”¨ threading backend é¿å… DEAP creator çš„ pickle å•é¡Œ
    results = Parallel(n_jobs=n_jobs, backend='threading')(
        delayed(calculate_ted_for_pair)(i, j, ind_i, ind_j)
        for i, j, ind_i, ind_j in pairs
    )
    
    # å¡«å……çŸ©é™£
    for i, j, ted in results:
        ted_matrix[i, j] = ted
        ted_matrix[j, i] = ted
    
    # è¨ˆç®—å¹³å‡è·é›¢ï¼ˆéå°è§’ç·šå…ƒç´ ï¼‰
    upper_tri = np.triu_indices(n, k=1)
    mean_ted = np.mean(ted_matrix[upper_tri])
    
    return mean_ted


def analyze_generation_diversity(generation: int,
                                 population: List,
                                 train_data_dict: Dict,
                                 backtest_start: str,
                                 backtest_end: str,
                                 sample_size: int = None,
                                 n_jobs: int = 4) -> Dict:
    """
    åˆ†æå–®å€‹ä¸–ä»£çš„å¤šæ¨£æ€§
    
    Args:
        generation: ä¸–ä»£è™Ÿ
        population: æ—ç¾¤
        train_data_dict: è¨“ç·´æ•¸æ“šå­—å…¸
        backtest_start: å›æ¸¬é–‹å§‹æ—¥æœŸ
        backtest_end: å›æ¸¬çµæŸæ—¥æœŸ
        sample_size: æ¡æ¨£å¤§å°
        n_jobs: å¹³è¡Œè™•ç†å™¨æ•¸é‡
        
    Returns:
        åŒ…å«å¤šæ¨£æ€§æŒ‡æ¨™çš„å­—å…¸
    """
    # æ¡æ¨£
    if sample_size and sample_size < len(population):
        population = sample_population(population, sample_size, 'stratified')
    
    # è¨ˆç®— PnL diversity
    mean_pnl_corr, valid_count, total_count = calculate_pnl_diversity(
        population, train_data_dict, backtest_start, backtest_end, n_jobs
    )
    
    # è¨ˆç®— TED diversity
    mean_ted_dist = calculate_ted_diversity(population, n_jobs)
    
    return {
        'generation': generation,
        'population_size': len(population),
        'valid_individuals': valid_count,
        'mean_pnl_correlation': mean_pnl_corr,
        'mean_ted_distance': mean_ted_dist
    }


def visualize_diversity_evolution(diversity_stats: List[Dict],
                                  output_path: Path):
    """
    è¦–è¦ºåŒ–å¤šæ¨£æ€§æ¼”åŒ–è¶¨å‹¢
    
    Args:
        diversity_stats: æ¯å€‹ä¸–ä»£çš„å¤šæ¨£æ€§çµ±è¨ˆ
        output_path: è¼¸å‡ºåœ–è¡¨è·¯å¾‘
    """
    # æå–æ•¸æ“š
    generations = [s['generation'] for s in diversity_stats]
    pnl_corrs = [s['mean_pnl_correlation'] for s in diversity_stats]
    ted_dists = [s['mean_ted_distance'] for s in diversity_stats]
    
    # å‰µå»ºåœ–è¡¨
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # ========== ä¸Šå­åœ–ï¼šPnL Correlation ==========
    ax_pnl = axes[0]
    
    ax_pnl.plot(generations, pnl_corrs, 
               marker='o', linewidth=2.5, markersize=8,
               color='#2E86AB', alpha=0.9, label='Mean PnL Correlation')
    
    ax_pnl.set_xlabel('Generation', fontsize=13, fontweight='bold')
    ax_pnl.set_ylabel('Mean PnL Correlation', fontsize=13, fontweight='bold')
    ax_pnl.set_title('Evolution of PnL Correlation (Phenotypic Diversity)', 
                    fontsize=15, fontweight='bold', pad=15)
    ax_pnl.grid(True, alpha=0.3, linestyle='--')
    ax_pnl.legend(loc='best', fontsize=11, framealpha=0.9)
    
    # æ·»åŠ è¶¨å‹¢ç·š
    z = np.polyfit(generations, pnl_corrs, 2)
    p = np.poly1d(z)
    ax_pnl.plot(generations, p(generations), 
               linestyle='--', color='red', alpha=0.5, linewidth=2,
               label='Trend (2nd order)')
    ax_pnl.legend(loc='best', fontsize=11, framealpha=0.9)
    
    # çµ±è¨ˆä¿¡æ¯
    stats_text = f'Range: [{min(pnl_corrs):.4f}, {max(pnl_corrs):.4f}] | Mean: {np.mean(pnl_corrs):.4f} | Std: {np.std(pnl_corrs):.4f}'
    ax_pnl.text(0.5, 0.02, stats_text, transform=ax_pnl.transAxes,
               ha='center', fontsize=10,
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    
    # ========== ä¸‹å­åœ–ï¼šTED Distance ==========
    ax_ted = axes[1]
    
    ax_ted.plot(generations, ted_dists,
               marker='s', linewidth=2.5, markersize=8,
               color='#A23B72', alpha=0.9, label='Mean TED Distance')
    
    ax_ted.set_xlabel('Generation', fontsize=13, fontweight='bold')
    ax_ted.set_ylabel('Mean Normalized TED Distance', fontsize=13, fontweight='bold')
    ax_ted.set_title('Evolution of TED Distance (Genotypic Diversity)', 
                    fontsize=15, fontweight='bold', pad=15)
    ax_ted.grid(True, alpha=0.3, linestyle='--')
    ax_ted.legend(loc='best', fontsize=11, framealpha=0.9)
    
    # æ·»åŠ è¶¨å‹¢ç·š
    z = np.polyfit(generations, ted_dists, 2)
    p = np.poly1d(z)
    ax_ted.plot(generations, p(generations),
               linestyle='--', color='red', alpha=0.5, linewidth=2,
               label='Trend (2nd order)')
    ax_ted.legend(loc='best', fontsize=11, framealpha=0.9)
    
    # çµ±è¨ˆä¿¡æ¯
    stats_text = f'Range: [{min(ted_dists):.4f}, {max(ted_dists):.4f}] | Mean: {np.mean(ted_dists):.4f} | Std: {np.std(ted_dists):.4f}'
    ax_ted.text(0.5, 0.02, stats_text, transform=ax_ted.transAxes,
               ha='center', fontsize=10,
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… åœ–è¡¨å·²ä¿å­˜: {output_path}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="è¦–è¦ºåŒ–å¤šæ¨£æ€§æ¼”åŒ–è¶¨å‹¢"
    )
    parser.add_argument(
        '--records',
        type=str,
        required=True,
        help='å¯¦é©—è¨˜éŒ„ç›®éŒ„è·¯å¾‘'
    )
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='é…ç½®æ–‡ä»¶è·¯å¾‘'
    )
    parser.add_argument(
        '--sample-size',
        type=int,
        default=None,
        help='æ¡æ¨£å¤§å°ï¼ˆé»˜èªä½¿ç”¨å…¨éƒ¨å€‹é«”ï¼‰'
    )
    parser.add_argument(
        '--n-jobs',
        type=int,
        default=4,
        help='å¹³è¡Œè™•ç†å™¨æ•¸é‡ï¼ˆé»˜èªï¼š4ï¼‰'
    )
    parser.add_argument(
        '--save-matrices',
        action='store_true',
        help='ä¿å­˜æ¯å€‹ä¸–ä»£çš„çŸ©é™£åˆ° CSV'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¼¸å‡ºåœ–è¡¨è·¯å¾‘ï¼ˆé»˜èªä¿å­˜åœ¨è¨˜éŒ„ç›®éŒ„ä¸­ï¼‰'
    )
    
    args = parser.parse_args()
    
    records_dir = Path(args.records)
    config_file = Path(args.config)
    
    if not records_dir.exists():
        print(f"âŒ è¨˜éŒ„ç›®éŒ„ä¸å­˜åœ¨: {records_dir}")
        return
    
    if not config_file.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return
    
    print("=" * 80)
    print("ğŸ¯ Diversity Evolution Analysis")
    print("=" * 80)
    print(f"Records directory: {records_dir}")
    print(f"Config file: {config_file}")
    print(f"Sample size: {args.sample_size if args.sample_size else 'All'}")
    print(f"N jobs: {args.n_jobs}\n")
    
    # 1. è¨­ç½® DEAP
    setup_deap_creator()
    
    # 2. è¼‰å…¥é…ç½®
    print("ğŸ“‹ è¼‰å…¥é…ç½®...")
    with open(config_file, 'r') as f:
        config = json.load(f)
    
    # 3. è¼‰å…¥æ‰€æœ‰ä¸–ä»£
    print("\nğŸ“¦ è¼‰å…¥æ‰€æœ‰ä¸–ä»£...")
    populations = load_all_generations(records_dir)
    
    if len(populations) == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•ä¸–ä»£æ•¸æ“š")
        return
    
    # 4. è¼‰å…¥æ•¸æ“š
    print("ğŸ“Š è¼‰å…¥æ•¸æ“š...")
    import os
    tickers_dir = config['data']['tickers_dir']
    ticker_files = [f for f in os.listdir(tickers_dir) if f.endswith('.csv')]
    tickers = [f.replace('.csv', '') for f in ticker_files]
    
    data = load_and_process_data(tickers_dir, tickers)
    train_data, test_data = split_train_test_data(
        data,
        train_data_start=config['data']['train_data_start'],
        train_backtest_start=config['data']['train_backtest_start'],
        train_backtest_end=config['data']['train_backtest_end'],
        test_data_start=config['data']['test_data_start'],
        test_backtest_start=config['data']['test_backtest_start'],
        test_backtest_end=config['data']['test_backtest_end']
    )
    print(f"âœ… è¼‰å…¥ {len(train_data)} å€‹è‚¡ç¥¨çš„æ•¸æ“š\n")
    
    # 5. æº–å‚™å›æ¸¬åƒæ•¸ï¼ˆå‚³éçµ¦å­é€²ç¨‹ï¼‰
    print("ğŸ—ï¸  æº–å‚™å›æ¸¬åƒæ•¸...")
    train_data_dict = {ticker: info['data'] for ticker, info in train_data.items()}
    backtest_start = config['data']['train_backtest_start']
    backtest_end = config['data']['train_backtest_end']
    print("âœ… åƒæ•¸æº–å‚™å®Œæˆ\n")
    
    # 6. åˆ†ææ¯å€‹ä¸–ä»£çš„å¤šæ¨£æ€§ï¼ˆé †åºè™•ç†ä¸–ä»£ï¼Œä½†æ¯å€‹ä¸–ä»£å…§éƒ¨å¹³è¡ŒåŒ–ï¼‰
    print("ğŸ”¬ åˆ†ææ¯å€‹ä¸–ä»£çš„å¤šæ¨£æ€§...")
    print("=" * 80)
    
    diversity_stats = []
    
    for gen_num in tqdm(sorted(populations.keys()), desc="è™•ç†ä¸–ä»£", ncols=80):
        population = populations[gen_num]
        
        stats = analyze_generation_diversity(
            gen_num,
            population,
            train_data_dict,
            backtest_start,
            backtest_end,
            args.sample_size,
            args.n_jobs
        )
        
        diversity_stats.append(stats)
        
        print(f"Gen {gen_num:3d}: PnL Corr={stats['mean_pnl_correlation']:.4f}, "
              f"TED Dist={stats['mean_ted_distance']:.4f}, "
              f"Valid={stats['valid_individuals']}/{stats['population_size']}")
    
    # 7. ä¿å­˜çµ±è¨ˆæ‘˜è¦
    print("\nğŸ’¾ ä¿å­˜çµ±è¨ˆæ‘˜è¦...")
    summary_df = pd.DataFrame(diversity_stats)
    summary_path = records_dir / 'diversity_evolution_summary.csv'
    summary_df.to_csv(summary_path, index=False)
    print(f"   âœ… æ‘˜è¦å·²ä¿å­˜: {summary_path}")
    
    # 8. è¦–è¦ºåŒ–
    print("\nğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
    
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = records_dir / 'diversity_evolution.png'
    
    visualize_diversity_evolution(diversity_stats, output_path)
    
    # 9. è¼¸å‡ºç¸½çµ
    print("\n" + "=" * 80)
    print("âœ… å®Œæˆ!")
    print("=" * 80)
    print(f"åˆ†æä¸–ä»£æ•¸: {len(diversity_stats)}")
    print(f"\nPnL Correlation è¶¨å‹¢:")
    print(f"  èµ·å§‹å€¼: {diversity_stats[0]['mean_pnl_correlation']:.4f}")
    print(f"  æœ€çµ‚å€¼: {diversity_stats[-1]['mean_pnl_correlation']:.4f}")
    print(f"  è®ŠåŒ–é‡: {diversity_stats[-1]['mean_pnl_correlation'] - diversity_stats[0]['mean_pnl_correlation']:.4f}")
    print(f"\nTED Distance è¶¨å‹¢:")
    print(f"  èµ·å§‹å€¼: {diversity_stats[0]['mean_ted_distance']:.4f}")
    print(f"  æœ€çµ‚å€¼: {diversity_stats[-1]['mean_ted_distance']:.4f}")
    print(f"  è®ŠåŒ–é‡: {diversity_stats[-1]['mean_ted_distance'] - diversity_stats[0]['mean_ted_distance']:.4f}")
    print(f"\nè¼¸å‡ºæ–‡ä»¶:")
    print(f"  - {output_path}")
    print(f"  - {summary_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
