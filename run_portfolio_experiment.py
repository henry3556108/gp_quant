"""
Phase 1: Portfolio-Based GP Evolution Experiment (with Norm Operator)

å¤šè‚¡ç¥¨çµ„åˆçš„ GP æ¼”åŒ–å¯¦é©—
- ä½¿ç”¨ PortfolioBacktestingEngine åŒæ™‚è©•ä¼°å¤šå€‹è‚¡ç¥¨
- åŒ…å«æ–°å¯¦ä½œçš„ Norm operator
- å„²å­˜æ¯å€‹ generation çš„æ—ç¾¤å¿«ç…§
- 5000 å€‹é«”ï¼Œ50 ä»£æ¼”åŒ–ï¼ˆå¤§è¦æ¨¡å¯¦é©—ï¼‰
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
from deap import creator, base, gp, tools
import random
import json
import os
import dill

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from gp_quant.backtesting.portfolio_engine import PortfolioBacktestingEngine
from gp_quant.gp.operators import pset
from gp_quant.evolution.early_stopping import EarlyStopping
from gp_quant.similarity import SimilarityMatrix
from gp_quant.niching import NichingClusterer, CrossNicheSelector

def main():
    print("="*100)
    print("ğŸš€ Phase 1: Portfolio-Based GP Evolution Experiment")
    print("="*100)
    print()
    
    # ============================================================================
    # å¯¦é©—é…ç½®
    # ============================================================================
    
    CONFIG = {
        # è‚¡ç¥¨çµ„åˆ
        'tickers': ['ABX.TO', 'BBD-B.TO', 'RY.TO', 'TRP.TO'],
        
        # è¨“ç·´æœŸï¼ˆæ¨£æœ¬å…§ï¼‰
        'train_data_start': '1995-01-03',
        'train_backtest_start': '1997-06-25',
        'train_backtest_end': '1999-06-25',
        
        # æ¸¬è©¦æœŸï¼ˆæ¨£æœ¬å¤–ï¼‰
        'test_data_start': '1997-06-25',
        'test_backtest_start': '1999-06-26',
        'test_backtest_end': '2001-06-26',
        
        'initial_capital': 100000.0,
        
        # GP åƒæ•¸
        'population_size': 100,
        'generations': 50,
        
        # æ¼”åŒ–åƒæ•¸
        'crossover_prob': 0.8,
        'mutation_prob': 0.2,
        'tournament_size': 3,
        
        # Fitness è¨ˆç®—æ–¹å¼
        'fitness_metric': 'sharpe_ratio',  # 'excess_return', 'sharpe_ratio', 'avg_sharpe'
        'risk_free_rate': 0.0,  # å¹´åŒ–ç„¡é¢¨éšªåˆ©ç‡
        
        # æ—©åœé…ç½®
        'early_stopping_enabled': True,      # æ˜¯å¦å•Ÿç”¨æ—©åœ
        'early_stopping_patience': 5,       # é€£çºŒç„¡é€²æ­¥çš„ä»£æ•¸
        'early_stopping_min_delta': 0.001,   # æœ€å°æ”¹é€²é–¾å€¼ï¼ˆæ ¹æ“š fitness_metric èª¿æ•´ï¼‰
        
        # Niching é…ç½®
        'niching_enabled': True,            # æ˜¯å¦å•Ÿç”¨ Niching ç­–ç•¥
        'niching_n_clusters': 5,            # Niche æ•¸é‡
        'niching_cross_ratio': 0.8,         # è·¨ç¾¤äº¤é…æ¯”ä¾‹ (0.8 = 80%)
        'niching_update_frequency': 5,      # æ¯ N ä»£é‡æ–°è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
        'niching_algorithm': 'kmeans',      # èšé¡æ¼”ç®—æ³• ('kmeans' æˆ– 'hierarchical')
        
        # è¼¸å‡ºç›®éŒ„
        'output_dir': 'portfolio_experiment_results',
        'experiment_name': f'portfolio_exp_sharpe_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    }
    
    print("ğŸ“‹ å¯¦é©—é…ç½®:")
    print(f"  è‚¡ç¥¨çµ„åˆ: {', '.join(CONFIG['tickers'])}")
    print(f"\n  è¨“ç·´æœŸï¼ˆæ¨£æœ¬å…§ï¼‰:")
    print(f"    æ•¸æ“šæœŸé–“: {CONFIG['train_data_start']} åˆ° {CONFIG['train_backtest_start']}")
    print(f"    å›æ¸¬æœŸé–“: {CONFIG['train_backtest_start']} åˆ° {CONFIG['train_backtest_end']}")
    print(f"\n  æ¸¬è©¦æœŸï¼ˆæ¨£æœ¬å¤–ï¼‰:")
    print(f"    æ•¸æ“šæœŸé–“: {CONFIG['test_data_start']} åˆ° {CONFIG['test_backtest_start']}")
    print(f"    å›æ¸¬æœŸé–“: {CONFIG['test_backtest_start']} åˆ° {CONFIG['test_backtest_end']}")
    print(f"\n  åˆå§‹è³‡é‡‘: ${CONFIG['initial_capital']:,.0f}")
    print(f"  æ—ç¾¤å¤§å°: {CONFIG['population_size']}")
    print(f"  æ¼”åŒ–ä¸–ä»£: {CONFIG['generations']}")
    print(f"  Fitness æŒ‡æ¨™: {CONFIG['fitness_metric']}")
    if CONFIG['early_stopping_enabled']:
        print(f"  æ—©åœæ©Ÿåˆ¶: å•Ÿç”¨ï¼ˆpatience={CONFIG['early_stopping_patience']}, min_delta={CONFIG['early_stopping_min_delta']}ï¼‰")
    else:
        print(f"  æ—©åœæ©Ÿåˆ¶: åœç”¨")
    if CONFIG['niching_enabled']:
        print(f"  Niching ç­–ç•¥: å•Ÿç”¨")
        print(f"    - Niche æ•¸é‡: {CONFIG['niching_n_clusters']}")
        print(f"    - è·¨ç¾¤æ¯”ä¾‹: {CONFIG['niching_cross_ratio']:.0%}")
        print(f"    - æ›´æ–°é »ç‡: æ¯ {CONFIG['niching_update_frequency']} ä»£")
        print(f"    - èšé¡æ¼”ç®—æ³•: {CONFIG['niching_algorithm']}")
    else:
        print(f"  Niching ç­–ç•¥: åœç”¨")
    print()
    
    # ============================================================================
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    # ============================================================================
    
    exp_dir = Path(CONFIG['output_dir']) / CONFIG['experiment_name']
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # å‰µå»ºå­ç›®éŒ„
    generations_dir = exp_dir / "generations"
    generations_dir.mkdir(exist_ok=True)
    
    logs_dir = exp_dir / "logs"
    logs_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {exp_dir}")
    print()
    
    # å„²å­˜é…ç½®
    with open(exp_dir / "config.json", 'w') as f:
        json.dump(CONFIG, f, indent=2)
    
    # ============================================================================
    # è¼‰å…¥æ•¸æ“š
    # ============================================================================
    
    print("1ï¸âƒ£  è¼‰å…¥å¸‚å ´æ•¸æ“š...")
    data = {}
    
    for ticker in CONFIG['tickers']:
        file_path = project_root / f"TSE300_selected/{ticker}.csv"
        if file_path.exists():
            df = pd.read_csv(file_path, index_col=0, parse_dates=True)
            data[ticker] = df
            print(f"   âœ“ {ticker}: {len(df)} å¤©")
        else:
            print(f"   âœ— {ticker}: æ–‡ä»¶ä¸å­˜åœ¨")
            sys.exit(1)
    
    print()
    
    # ============================================================================
    # å‰µå»ºè¨“ç·´å’Œæ¸¬è©¦ Engine
    # ============================================================================
    
    print("2ï¸âƒ£  åˆå§‹åŒ– Portfolio Backtesting Engines...")
    
    # è¨“ç·´ Engineï¼ˆæ¨£æœ¬å…§ï¼‰
    print("\n   è¨“ç·´æœŸ Engine:")
    try:
        train_engine = PortfolioBacktestingEngine(
            data=data,
            backtest_start=CONFIG['train_backtest_start'],
            backtest_end=CONFIG['train_backtest_end'],
            initial_capital=CONFIG['initial_capital'],
            pset=pset
        )
        print(f"     âœ“ åˆå§‹åŒ–æˆåŠŸ")
        print(f"     âœ“ äº¤æ˜“æ—¥æ•¸: {len(train_engine.common_dates)}")
        print(f"     âœ“ æ—¥æœŸç¯„åœ: {train_engine.common_dates[0].date()} åˆ° {train_engine.common_dates[-1].date()}")
    except Exception as e:
        print(f"     âœ— åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # æ¸¬è©¦ Engineï¼ˆæ¨£æœ¬å¤–ï¼‰
    print("\n   æ¸¬è©¦æœŸ Engine:")
    try:
        test_engine = PortfolioBacktestingEngine(
            data=data,
            backtest_start=CONFIG['test_backtest_start'],
            backtest_end=CONFIG['test_backtest_end'],
            initial_capital=CONFIG['initial_capital'],
            pset=pset
        )
        print(f"     âœ“ åˆå§‹åŒ–æˆåŠŸ")
        print(f"     âœ“ äº¤æ˜“æ—¥æ•¸: {len(test_engine.common_dates)}")
        print(f"     âœ“ æ—¥æœŸç¯„åœ: {test_engine.common_dates[0].date()} åˆ° {test_engine.common_dates[-1].date()}")
    except Exception as e:
        print(f"     âœ— åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    print()
    
    # ============================================================================
    # è¨­ç½® DEAP
    # ============================================================================
    
    print("3ï¸âƒ£  è¨­ç½® DEAP...")
    
    # å‰µå»º Fitness å’Œ Individual é¡å‹
    if not hasattr(creator, "FitnessMax"):
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
    if not hasattr(creator, "Individual"):
        creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)
    
    # å‰µå»º toolbox
    toolbox = base.Toolbox()
    
    # è¨»å†Š GP æ“ä½œ
    toolbox.register("expr", gp.genHalfAndHalf, pset=pset, min_=1, max_=3)
    toolbox.register("individual", tools.initIterate, creator.Individual, toolbox.expr)
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)
    toolbox.register("compile", gp.compile, pset=pset)
    
    # è¨»å†Šæ¼”åŒ–æ“ä½œ
    toolbox.register("select", tools.selTournament, tournsize=CONFIG['tournament_size'])
    toolbox.register("mate", gp.cxOnePoint)
    toolbox.register("mutate", gp.mutUniform, expr=toolbox.expr, pset=pset)
    
    # å®šç¾© fitness è©•ä¼°å‡½æ•¸ï¼ˆä½¿ç”¨è¨“ç·´æœŸæ•¸æ“šï¼‰
    def evaluate_individual(individual):
        """è©•ä¼°å–®å€‹å€‹é«”çš„ fitnessï¼ˆè¨“ç·´æœŸï¼‰"""
        try:
            fitness = train_engine.get_fitness(individual, fitness_metric=CONFIG['fitness_metric'])
            return (fitness,)
        except Exception as e:
            return (-1000000.0,)
    
    toolbox.register("evaluate", evaluate_individual)
    
    print("   âœ“ DEAP è¨­ç½®å®Œæˆ")
    print()
    
    # ============================================================================
    # å‰µå»ºçµ±è¨ˆå’Œ Hall of Fame
    # ============================================================================
    
    stats = tools.Statistics(lambda ind: ind.fitness.values)
    stats.register("avg", np.mean)
    stats.register("std", np.std)
    stats.register("min", np.min)
    stats.register("max", np.max)
    
    hof = tools.HallOfFame(10)  # ä¿å­˜å‰ 10 å€‹æœ€ä½³å€‹é«”
    
    # ============================================================================
    # å‰µå»ºåˆå§‹æ—ç¾¤
    # ============================================================================
    
    print("4ï¸âƒ£  å‰µå»ºåˆå§‹æ—ç¾¤...")
    population = toolbox.population(n=CONFIG['population_size'])
    print(f"   âœ“ å‰µå»º {len(population)} å€‹å€‹é«”")
    print()
    
    # ============================================================================
    # é–‹å§‹æ¼”åŒ–
    # ============================================================================
    
    print("5ï¸âƒ£  é–‹å§‹ GP æ¼”åŒ–...")
    print(f"   æ—ç¾¤å¤§å°: {CONFIG['population_size']}")
    print(f"   æ¼”åŒ–ä¸–ä»£: {CONFIG['generations']}")
    print()
    print("="*100)
    
    # è¨˜éŒ„æ¼”åŒ–æ­·å²
    evolution_log = []
    start_time = datetime.now()
    
    # åˆå§‹åŒ–æ—©åœæ©Ÿåˆ¶
    early_stopping = None
    if CONFIG['early_stopping_enabled']:
        early_stopping = EarlyStopping(
            patience=CONFIG['early_stopping_patience'],
            min_delta=CONFIG['early_stopping_min_delta'],
            mode='max'  # Fitness è¶Šå¤§è¶Šå¥½
        )
        print(f"âœ“ æ—©åœæ©Ÿåˆ¶å·²å•Ÿç”¨ï¼ˆpatience={CONFIG['early_stopping_patience']}, min_delta={CONFIG['early_stopping_min_delta']}ï¼‰")
        print()
    
    # åˆå§‹åŒ– Niching æ©Ÿåˆ¶
    niching_selector = None
    niche_labels = None
    niching_log = []
    
    if CONFIG['niching_enabled']:
        niching_selector = CrossNicheSelector(
            cross_niche_ratio=CONFIG['niching_cross_ratio'],
            tournament_size=CONFIG['tournament_size'],
            random_state=42
        )
        print(f"âœ“ Niching ç­–ç•¥å·²å•Ÿç”¨")
        print(f"  - Niche æ•¸é‡: {CONFIG['niching_n_clusters']}")
        print(f"  - è·¨ç¾¤æ¯”ä¾‹: {CONFIG['niching_cross_ratio']:.0%}")
        print(f"  - æ›´æ–°é »ç‡: æ¯ {CONFIG['niching_update_frequency']} ä»£")
        print()
    
    for gen in range(CONFIG['generations']):
        gen_start_time = datetime.now()
        
        print(f"\n{'='*100}")
        print(f"ğŸ“Š Generation {gen + 1}/{CONFIG['generations']}")
        print(f"{'='*100}")
        
        # è©•ä¼°æ—ç¾¤
        print(f"â³ è©•ä¼° {len(population)} å€‹å€‹é«”...")
        eval_start = datetime.now()
        
        # è©•ä¼°æ‰€æœ‰å€‹é«”
        invalid_ind = [ind for ind in population if not ind.fitness.valid]
        fitnesses = map(toolbox.evaluate, invalid_ind)
        for ind, fit in zip(invalid_ind, fitnesses):
            ind.fitness.values = fit
        
        eval_time = (datetime.now() - eval_start).total_seconds()
        print(f"âœ“ è©•ä¼°å®Œæˆ ({eval_time:.1f}s)")
        
        # æ›´æ–°çµ±è¨ˆ
        hof.update(population)
        record = stats.compile(population)
        
        # é¡¯ç¤ºçµ±è¨ˆ
        min_fit = record['min']
        avg_fit = record['avg']
        max_fit = record['max']
        std_fit = record['std']
        
        print(f"\nğŸ“ˆ Fitness çµ±è¨ˆ:")
        print(f"   Min: {min_fit:.4f} ({min_fit*100:+.2f}%) | PnL: ${min_fit*CONFIG['initial_capital']:+,.0f}")
        print(f"   Avg: {avg_fit:.4f} ({avg_fit*100:+.2f}%) | PnL: ${avg_fit*CONFIG['initial_capital']:+,.0f}")
        print(f"   Max: {max_fit:.4f} ({max_fit*100:+.2f}%) | PnL: ${max_fit*CONFIG['initial_capital']:+,.0f}")
        print(f"   Std: {std_fit:.4f}")
        
        # è¨˜éŒ„åˆ°æ—¥èªŒ
        gen_log = {
            'generation': gen + 1,
            'min_fitness': float(min_fit),
            'avg_fitness': float(avg_fit),
            'max_fitness': float(max_fit),
            'std_fitness': float(std_fit),
            'eval_time': eval_time,
            'timestamp': datetime.now().isoformat()
        }
        evolution_log.append(gen_log)
        
        # ========================================================================
        # æ—©åœæª¢æŸ¥
        # ========================================================================
        
        if early_stopping is not None:
            current_best = hof[0].fitness.values[0]
            
            if early_stopping.step(current_best):
                print(f"\nâ¹ï¸  æ—©åœè§¸ç™¼ï¼")
                print(f"   é€£çºŒ {early_stopping.counter} ä»£ç„¡é¡¯è‘—é€²æ­¥")
                print(f"   æœ€ä½³ fitness: {early_stopping.best_fitness:.4f}")
                print(f"   æœ€çµ‚ generation: {gen + 1}/{CONFIG['generations']}")
                print(f"   æ—©åœç‹€æ…‹: {early_stopping.get_status()}")
                
                # è¨˜éŒ„æ—©åœè³‡è¨Š
                gen_log['early_stopped'] = True
                gen_log['early_stop_reason'] = f'No improvement for {early_stopping.counter} generations'
                
                # å„²å­˜æœ€å¾Œä¸€ä»£å¾Œè·³å‡ºå¾ªç’°
                print(f"\nğŸ’¾ å„²å­˜æœ€çµ‚ Generation {gen + 1} æ—ç¾¤...")
                gen_file = generations_dir / f"generation_{gen+1:03d}_final.pkl"
                
                try:
                    with open(gen_file, 'wb') as f:
                        dill.dump({
                            'generation': gen + 1,
                            'population': population,
                            'hall_of_fame': list(hof),
                            'statistics': record,
                            'early_stopped': True,
                            'early_stopping_status': early_stopping.get_status(),
                            'timestamp': datetime.now().isoformat()
                        }, f)
                    
                    file_size = gen_file.stat().st_size / (1024 * 1024)
                    print(f"   âœ“ å·²å„²å­˜: {gen_file.name} ({file_size:.2f} MB)")
                except Exception as e:
                    print(f"   âœ— å„²å­˜å¤±æ•—: {e}")
                
                break  # è·³å‡ºæ¼”åŒ–å¾ªç’°
            else:
                # é¡¯ç¤ºæ—©åœç‹€æ…‹
                if gen > 0:  # ç¬¬ä¸€ä»£ä¸é¡¯ç¤º
                    print(f"\nâ¸ï¸  æ—©åœç‹€æ…‹: {early_stopping.counter}/{early_stopping.patience} ä»£ç„¡é€²æ­¥")
        
        # ========================================================================
        # å„²å­˜ç•¶å‰ä¸–ä»£çš„æ—ç¾¤
        # ========================================================================
        
        print(f"\nğŸ’¾ å„²å­˜ Generation {gen + 1} æ—ç¾¤...")
        gen_file = generations_dir / f"generation_{gen+1:03d}.pkl"
        
        try:
            # å„²å­˜æ•´å€‹æ—ç¾¤
            with open(gen_file, 'wb') as f:
                dill.dump({
                    'generation': gen + 1,
                    'population': population,
                    'hall_of_fame': list(hof),
                    'statistics': record,
                    'timestamp': datetime.now().isoformat()
                }, f)
            
            file_size = gen_file.stat().st_size / (1024 * 1024)  # MB
            print(f"   âœ“ å·²å„²å­˜: {gen_file.name} ({file_size:.2f} MB)")
            
        except Exception as e:
            print(f"   âœ— å„²å­˜å¤±æ•—: {e}")
        
        # é¡¯ç¤ºæœ€ä½³å€‹é«”
        best_ind = hof[0]
        print(f"\nğŸ† ç•¶å‰æœ€ä½³å€‹é«”:")
        print(f"   Fitness: {best_ind.fitness.values[0]:.4f} ({best_ind.fitness.values[0]*100:+.2f}%)")
        print(f"   PnL: ${best_ind.fitness.values[0]*CONFIG['initial_capital']:+,.0f}")
        print(f"   æ·±åº¦: {best_ind.height}, ç¯€é»æ•¸: {len(best_ind)}")
        print(f"   è¦å‰‡: {str(best_ind)[:100]}{'...' if len(str(best_ind)) > 100 else ''}")
        
        # ========================================================================
        # é¸æ“‡å’Œç¹æ®–ï¼ˆå¦‚æœä¸æ˜¯æœ€å¾Œä¸€ä»£ï¼‰
        # ========================================================================
        
        if gen < CONFIG['generations'] - 1:
            print(f"\nğŸ”„ é¸æ“‡å’Œç¹æ®–...")
            
            # ====================================================================
            # Niching: è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£ä¸¦èšé¡ï¼ˆæ¯ N ä»£æ›´æ–°ä¸€æ¬¡ï¼‰
            # ====================================================================
            if CONFIG['niching_enabled'] and gen % CONFIG['niching_update_frequency'] == 0:
                print(f"\nğŸ”¬ Niching: è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£...")
                sim_start = datetime.now()
                
                try:
                    sim_matrix = SimilarityMatrix(population)
                    similarity_matrix = sim_matrix.compute(show_progress=False)
                    sim_time = (datetime.now() - sim_start).total_seconds()
                    
                    print(f"   âœ“ ç›¸ä¼¼åº¦çŸ©é™£è¨ˆç®—å®Œæˆ ({sim_time:.1f}s)")
                    print(f"   å¹³å‡ç›¸ä¼¼åº¦: {sim_matrix.get_average_similarity():.4f}")
                    print(f"   å¤šæ¨£æ€§åˆ†æ•¸: {sim_matrix.get_diversity_score():.4f}")
                    
                    # èšé¡
                    print(f"\nğŸ”¬ Niching: èšé¡ï¼ˆk={CONFIG['niching_n_clusters']}ï¼‰...")
                    clusterer = NichingClusterer(
                        n_clusters=CONFIG['niching_n_clusters'],
                        algorithm=CONFIG['niching_algorithm']
                    )
                    niche_labels = clusterer.fit_predict(similarity_matrix)
                    
                    print(f"   âœ“ èšé¡å®Œæˆ")
                    print(f"   Silhouette åˆ†æ•¸: {clusterer.silhouette_score_:.4f}")
                    
                    # çµ±è¨ˆå„ niche å¤§å°
                    unique_niches, counts = np.unique(niche_labels, return_counts=True)
                    print(f"   å„ Niche å¤§å°: {dict(zip(unique_niches, counts))}")
                    
                    # è¨˜éŒ„ niching çµ±è¨ˆ
                    niching_log.append({
                        'generation': gen + 1,
                        'avg_similarity': float(sim_matrix.get_average_similarity()),
                        'diversity_score': float(sim_matrix.get_diversity_score()),
                        'silhouette_score': float(clusterer.silhouette_score_),
                        'niche_sizes': {int(k): int(v) for k, v in zip(unique_niches, counts)},
                        'computation_time': sim_time
                    })
                    
                except Exception as e:
                    print(f"   âœ— Niching è¨ˆç®—å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¤±æ•—æ™‚ä½¿ç”¨å‚³çµ±é¸æ“‡
                    niche_labels = None
            
            # ====================================================================
            # Selection: ä½¿ç”¨ Niching æˆ–å‚³çµ±é¸æ“‡
            # ====================================================================
            if CONFIG['niching_enabled'] and niche_labels is not None:
                # ä½¿ç”¨è·¨ç¾¤é¸æ“‡
                print(f"\nğŸ¯ ä½¿ç”¨è·¨ç¾¤é¸æ“‡...")
                try:
                    offspring = niching_selector.select(population, niche_labels, len(population))
                    offspring = list(map(toolbox.clone, offspring))
                    
                    # é¡¯ç¤ºé¸æ“‡çµ±è¨ˆ
                    selection_stats = niching_selector.get_statistics()
                    print(f"   âœ“ é¸æ“‡å®Œæˆ")
                    print(f"   è·¨ç¾¤é…å°: {selection_stats['cross_niche_pairs']} ({selection_stats['cross_niche_ratio_actual']:.0%})")
                    print(f"   ç¾¤å…§é…å°: {selection_stats['within_niche_pairs']} ({selection_stats['within_niche_ratio_actual']:.0%})")
                    
                    # è¨˜éŒ„é¸æ“‡çµ±è¨ˆ
                    gen_log['niching_selection'] = {
                        'cross_niche_pairs': selection_stats['cross_niche_pairs'],
                        'within_niche_pairs': selection_stats['within_niche_pairs'],
                        'cross_niche_ratio': selection_stats['cross_niche_ratio_actual']
                    }
                    
                except Exception as e:
                    print(f"   âœ— è·¨ç¾¤é¸æ“‡å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¤±æ•—æ™‚ä½¿ç”¨å‚³çµ±é¸æ“‡
                    offspring = toolbox.select(population, len(population))
                    offspring = list(map(toolbox.clone, offspring))
            else:
                # ä½¿ç”¨å‚³çµ± tournament selection
                offspring = toolbox.select(population, len(population))
                offspring = list(map(toolbox.clone, offspring))
            
            # Crossover
            for child1, child2 in zip(offspring[::2], offspring[1::2]):
                if random.random() < CONFIG['crossover_prob']:
                    toolbox.mate(child1, child2)
                    del child1.fitness.values
                    del child2.fitness.values
            
            # Mutation
            for mutant in offspring:
                if random.random() < CONFIG['mutation_prob']:
                    toolbox.mutate(mutant)
                    del mutant.fitness.values
            
            population[:] = offspring
            print(f"   âœ“ æ–°ä¸€ä»£æ—ç¾¤å·²æº–å‚™")
        
        # é¡¯ç¤ºä¸–ä»£è€—æ™‚
        gen_time = (datetime.now() - gen_start_time).total_seconds()
        print(f"\nâ±ï¸  Generation {gen + 1} è€—æ™‚: {gen_time:.1f}s")
    
    # ============================================================================
    # æ¼”åŒ–å®Œæˆ
    # ============================================================================
    
    total_time = (datetime.now() - start_time).total_seconds()
    actual_generations = gen + 1  # å¯¦éš›é‹è¡Œçš„ä»£æ•¸
    
    print()
    print("="*100)
    print("âœ… æ¼”åŒ–å®Œæˆï¼")
    print("="*100)
    print()
    
    print(f"â±ï¸  ç¸½è€—æ™‚: {total_time/60:.2f} åˆ†é˜ ({total_time:.1f} ç§’)")
    print(f"ğŸ“Š ç¸½ä¸–ä»£æ•¸: {actual_generations}/{CONFIG['generations']}")
    
    # é¡¯ç¤ºæ—©åœè³‡è¨Š
    if early_stopping is not None and early_stopping.should_stop:
        print(f"â¹ï¸  æ—©åœ: æ˜¯ï¼ˆç¬¬ {actual_generations} ä»£è§¸ç™¼ï¼‰")
        print(f"   åŸå› : é€£çºŒ {early_stopping.patience} ä»£ç„¡é¡¯è‘—é€²æ­¥ï¼ˆmin_delta={early_stopping.min_delta}ï¼‰")
        print(f"   æœ€ä½³ fitness: {early_stopping.best_fitness:.4f}")
    else:
        print(f"â¹ï¸  æ—©åœ: å¦ï¼ˆå®Œæ•´é‹è¡Œï¼‰")
    
    print(f"âš¡ å¹³å‡æ¯ä»£: {total_time/actual_generations:.1f} ç§’")
    print()
    
    # ============================================================================
    # å„²å­˜æ¼”åŒ–æ—¥èªŒ
    # ============================================================================
    
    print("ğŸ’¾ å„²å­˜æ¼”åŒ–æ—¥èªŒ...")
    
    # å„²å­˜ JSON æ—¥èªŒ
    log_file = exp_dir / "evolution_log.json"
    log_data = {
        'config': CONFIG,
        'evolution_log': evolution_log,
        'total_time': total_time,
        'actual_generations': actual_generations,
        'final_statistics': {
            'best_fitness': float(hof[0].fitness.values[0]),
            'best_pnl': float(hof[0].fitness.values[0] * CONFIG['initial_capital'])
        }
    }
    
    # æ·»åŠ æ—©åœè³‡è¨Š
    if early_stopping is not None:
        log_data['early_stopping'] = {
            'enabled': True,
            'triggered': early_stopping.should_stop,
            'status': early_stopping.get_status()
        }
    else:
        log_data['early_stopping'] = {
            'enabled': False,
            'triggered': False
        }
    
    # æ·»åŠ  Niching è³‡è¨Š
    if CONFIG['niching_enabled']:
        log_data['niching'] = {
            'enabled': True,
            'n_clusters': CONFIG['niching_n_clusters'],
            'cross_ratio': CONFIG['niching_cross_ratio'],
            'update_frequency': CONFIG['niching_update_frequency'],
            'algorithm': CONFIG['niching_algorithm'],
            'log': niching_log
        }
    else:
        log_data['niching'] = {
            'enabled': False
        }
    
    with open(log_file, 'w') as f:
        json.dump(log_data, f, indent=2)
    print(f"   âœ“ {log_file}")
    
    # å„²å­˜ CSV æ—¥èªŒ
    log_df = pd.DataFrame(evolution_log)
    csv_file = exp_dir / "evolution_log.csv"
    log_df.to_csv(csv_file, index=False)
    print(f"   âœ“ {csv_file}")
    
    # ============================================================================
    # æœ€çµ‚åˆ†æ
    # ============================================================================
    
    print()
    print("="*100)
    print("ğŸ“Š æœ€çµ‚åˆ†æ")
    print("="*100)
    print()
    
    print("ğŸ† Top 10 æœ€ä½³å€‹é«”:")
    for i, ind in enumerate(hof, 1):
        fitness = ind.fitness.values[0]
        pnl = fitness * CONFIG['initial_capital']
        print(f"   {i:2d}. Fitness: {fitness:+.4f} ({fitness*100:+.2f}%) | "
              f"PnL: ${pnl:+,.0f} | "
              f"æ·±åº¦: {ind.height} | ç¯€é»: {len(ind)}")
    
    print()
    
    # è©³ç´°å›æ¸¬æœ€ä½³å€‹é«”
    print("ğŸ” è©³ç´°å›æ¸¬æœ€ä½³å€‹é«”...")
    best_individual = hof[0]
    
    # ========================================================================
    # è¨“ç·´æœŸï¼ˆæ¨£æœ¬å…§ï¼‰å›æ¸¬
    # ========================================================================
    print("\nğŸ“Š è¨“ç·´æœŸï¼ˆæ¨£æœ¬å…§ï¼‰ç¸¾æ•ˆ:")
    print("="*80)
    
    try:
        train_result = train_engine.backtest(best_individual)
        train_metrics = train_result['metrics']
        train_per_stock_pnl = train_result['per_stock_pnl']
        
        print(f"\nçµ„åˆç¸¾æ•ˆ:")
        print(f"  Total Return: {train_metrics['total_return']*100:.2f}%")
        print(f"  Sharpe Ratio: {train_metrics['sharpe_ratio']:.3f}")
        print(f"  Max Drawdown: {train_metrics['max_drawdown']*100:.2f}%")
        print(f"  Volatility: {train_metrics['volatility']*100:.2f}%")
        print(f"  Win Rate: {train_metrics['win_rate']*100:.2f}%")
        
        print(f"\nå„è‚¡ç¥¨ PnL è²¢ç»:")
        for ticker, pnl in train_per_stock_pnl.items():
            status = "âœ…" if pnl > 0 else "âŒ"
            print(f"  {ticker}: ${pnl:+,.2f} {status}")
        
        train_transactions = train_result['transactions']
        if len(train_transactions) > 0:
            buy_trades = len(train_transactions[train_transactions['action'] == 'BUY'])
            sell_trades = len(train_transactions[train_transactions['action'] == 'SELL'])
            print(f"\näº¤æ˜“çµ±è¨ˆ: ç¸½æ•¸ {len(train_transactions)} (è²·å…¥: {buy_trades}, è³£å‡º: {sell_trades})")
            
            # å„²å­˜è¨“ç·´æœŸäº¤æ˜“è¨˜éŒ„
            train_trades_file = exp_dir / "best_individual_train_trades.csv"
            train_transactions.to_csv(train_trades_file, index=False)
        
    except Exception as e:
        print(f"   âœ— è¨“ç·´æœŸå›æ¸¬å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    # ========================================================================
    # æ¸¬è©¦æœŸï¼ˆæ¨£æœ¬å¤–ï¼‰å›æ¸¬
    # ========================================================================
    print("\nğŸ“Š æ¸¬è©¦æœŸï¼ˆæ¨£æœ¬å¤–ï¼‰ç¸¾æ•ˆ:")
    print("="*80)
    
    try:
        test_result = test_engine.backtest(best_individual)
        test_metrics = test_result['metrics']
        test_per_stock_pnl = test_result['per_stock_pnl']
        
        print(f"\nçµ„åˆç¸¾æ•ˆ:")
        print(f"  Total Return: {test_metrics['total_return']*100:.2f}%")
        print(f"  Sharpe Ratio: {test_metrics['sharpe_ratio']:.3f}")
        print(f"  Max Drawdown: {test_metrics['max_drawdown']*100:.2f}%")
        print(f"  Volatility: {test_metrics['volatility']*100:.2f}%")
        print(f"  Win Rate: {test_metrics['win_rate']*100:.2f}%")
        
        print(f"\nå„è‚¡ç¥¨ PnL è²¢ç»:")
        for ticker, pnl in test_per_stock_pnl.items():
            status = "âœ…" if pnl > 0 else "âŒ"
            print(f"  {ticker}: ${pnl:+,.2f} {status}")
        
        test_transactions = test_result['transactions']
        if len(test_transactions) > 0:
            buy_trades = len(test_transactions[test_transactions['action'] == 'BUY'])
            sell_trades = len(test_transactions[test_transactions['action'] == 'SELL'])
            print(f"\näº¤æ˜“çµ±è¨ˆ: ç¸½æ•¸ {len(test_transactions)} (è²·å…¥: {buy_trades}, è³£å‡º: {sell_trades})")
            
            # å„²å­˜æ¸¬è©¦æœŸäº¤æ˜“è¨˜éŒ„
            test_trades_file = exp_dir / "best_individual_test_trades.csv"
            test_transactions.to_csv(test_trades_file, index=False)
        
        # ====================================================================
        # æ¯”è¼ƒè¨“ç·´æœŸ vs æ¸¬è©¦æœŸ
        # ====================================================================
        print("\nğŸ“ˆ è¨“ç·´æœŸ vs æ¸¬è©¦æœŸæ¯”è¼ƒ:")
        print("="*80)
        print(f"  {'æŒ‡æ¨™':<20} {'è¨“ç·´æœŸ':>15} {'æ¸¬è©¦æœŸ':>15} {'å·®ç•°':>15}")
        print(f"  {'-'*20} {'-'*15} {'-'*15} {'-'*15}")
        print(f"  {'Total Return':<20} {train_metrics['total_return']*100:>14.2f}% {test_metrics['total_return']*100:>14.2f}% {(test_metrics['total_return']-train_metrics['total_return'])*100:>+14.2f}%")
        print(f"  {'Sharpe Ratio':<20} {train_metrics['sharpe_ratio']:>15.3f} {test_metrics['sharpe_ratio']:>15.3f} {test_metrics['sharpe_ratio']-train_metrics['sharpe_ratio']:>+15.3f}")
        print(f"  {'Max Drawdown':<20} {train_metrics['max_drawdown']*100:>14.2f}% {test_metrics['max_drawdown']*100:>14.2f}% {(test_metrics['max_drawdown']-train_metrics['max_drawdown'])*100:>+14.2f}%")
        print(f"  {'Volatility':<20} {train_metrics['volatility']*100:>14.2f}% {test_metrics['volatility']*100:>14.2f}% {(test_metrics['volatility']-train_metrics['volatility'])*100:>+14.2f}%")
        
        # å„²å­˜å®Œæ•´çµæœ
        best_result_file = exp_dir / "best_individual_result.json"
        with open(best_result_file, 'w') as f:
            json.dump({
                'individual': str(best_individual),
                'train_fitness': float(best_individual.fitness.values[0]),
                'train_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else v 
                               for k, v in train_metrics.items()},
                'train_per_stock_pnl': {k: float(v) for k, v in train_per_stock_pnl.items()},
                'train_total_trades': len(train_transactions),
                'test_metrics': {k: float(v) if isinstance(v, (int, float, np.number)) else v 
                              for k, v in test_metrics.items()},
                'test_per_stock_pnl': {k: float(v) for k, v in test_per_stock_pnl.items()},
                'test_total_trades': len(test_transactions)
            }, f, indent=2)
        
        print(f"\nğŸ’¾ çµæœå·²å„²å­˜:")
        print(f"   âœ“ {best_result_file}")
        if len(train_transactions) > 0:
            print(f"   âœ“ {train_trades_file}")
        if len(test_transactions) > 0:
            print(f"   âœ“ {test_trades_file}")
        
    except Exception as e:
        print(f"   âœ— å›æ¸¬å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    print()
    print("="*100)
    print("ğŸ‰ å¯¦é©—å®Œæˆï¼")
    print("="*100)
    print()
    
    print(f"ğŸ“ æ‰€æœ‰çµæœå·²å„²å­˜è‡³: {exp_dir}")
    print(f"\nç›®éŒ„çµæ§‹:")
    print(f"  {exp_dir}/")
    print(f"  â”œâ”€â”€ config.json                         (å¯¦é©—é…ç½®)")
    print(f"  â”œâ”€â”€ evolution_log.json                  (æ¼”åŒ–æ—¥èªŒ)")
    print(f"  â”œâ”€â”€ evolution_log.csv                   (æ¼”åŒ–æ—¥èªŒ CSV)")
    print(f"  â”œâ”€â”€ best_individual_result.json         (æœ€ä½³å€‹é«”å®Œæ•´çµæœ)")
    print(f"  â”œâ”€â”€ best_individual_train_trades.csv    (è¨“ç·´æœŸäº¤æ˜“è¨˜éŒ„)")
    print(f"  â”œâ”€â”€ best_individual_test_trades.csv     (æ¸¬è©¦æœŸäº¤æ˜“è¨˜éŒ„)")
    print(f"  â””â”€â”€ generations/                        (æ—ç¾¤å¿«ç…§)")
    print(f"      â”œâ”€â”€ generation_001.pkl")
    print(f"      â”œâ”€â”€ generation_002.pkl")
    print(f"      â”œâ”€â”€ ...")
    print(f"      â””â”€â”€ generation_{CONFIG['generations']:03d}.pkl")
    print()

if __name__ == '__main__':
    main()
