"""
Test PnL Diversity Calculation

å¾žå¯¦é©—çµæžœä¸­é¸æ“‡ 5 å€‹å€‹é«”ï¼Œè¨ˆç®—ä»–å€‘çš„ PnL ç›¸é—œæ€§çŸ©é™£ä¸¦è¦–è¦ºåŒ–
"""

import pickle
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from deap import creator, base, gp
from typing import List, Dict, Any

from gp_quant.evolution.components.gp import operators  # å°Žå…¥ä»¥é…ç½® primitive set
from gp_quant.evolution.components.backtesting import PortfolioBacktestingEngine
from gp_quant.data.loader import load_and_process_data, split_train_test_data


def setup_deap_creator():
    """è¨­ç½® DEAP creator"""
    if not hasattr(creator, "FitnessMax"):
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
    if not hasattr(creator, "Individual"):
        creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)


def load_individuals_from_generation(records_dir: Path, generation: int) -> List:
    """å¾žæŒ‡å®šä¸–ä»£è¼‰å…¥å€‹é«”"""
    pkl_file = records_dir / "populations" / f"generation_{generation:03d}.pkl"
    
    if not pkl_file.exists():
        raise FileNotFoundError(f"Generation file not found: {pkl_file}")
    
    with open(pkl_file, 'rb') as f:
        population = pickle.load(f)
    
    print(f"âœ… è¼‰å…¥ä¸–ä»£ {generation}: {len(population)} å€‹å€‹é«”")
    return population


def select_diverse_individuals(population: List, n: int = 5) -> List:
    """
    é¸æ“‡å¤šæ¨£åŒ–çš„å€‹é«”
    
    ç­–ç•¥ï¼šé¸æ“‡ä¸åŒé©æ‡‰åº¦ç¯„åœçš„å€‹é«”
    """
    # æŒ‰é©æ‡‰åº¦æŽ’åº
    sorted_pop = sorted(population, key=lambda x: x.fitness.values[0], reverse=True)
    
    # é¸æ“‡åˆ†ä½ˆåœ¨ä¸åŒé©æ‡‰åº¦å€é–“çš„å€‹é«”
    indices = np.linspace(0, len(sorted_pop) - 1, n, dtype=int)
    selected = [sorted_pop[i] for i in indices]
    
    print(f"\nðŸ“Š é¸æ“‡çš„ {n} å€‹å€‹é«”:")
    for i, ind in enumerate(selected):
        print(f"   {i+1}. Fitness: {ind.fitness.values[0]:.4f}, Size: {len(ind)}, Height: {ind.height}")
    
    return selected


def calculate_pnl_curves(individuals: List, train_data: Dict, backtest_config: Dict) -> tuple:
    """è¨ˆç®—å€‹é«”çš„ PnL æ›²ç·š"""
    # PortfolioBacktestingEngine éœ€è¦æ¯å€‹ ticker çš„æ•¸æ“šå­—å…¸
    # å¾ž train_data æå–å¯¦éš›æ•¸æ“š
    data_dict = {ticker: info['data'] for ticker, info in train_data.items()}
    
    # åˆå§‹åŒ–å›žæ¸¬å¼•æ“Ž
    engine = PortfolioBacktestingEngine(
        data=data_dict,
        backtest_start=backtest_config['backtest_start'],
        backtest_end=backtest_config['backtest_end'],
        initial_capital=100000.0
    )
    
    pnl_curves = []
    valid_individuals = []
    
    print(f"\nðŸ’° è¨ˆç®— PnL æ›²ç·š...")
    
    for i, individual in enumerate(individuals):
        try:
            pnl_curve = engine.get_pnl_curve(individual)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯å…¨é›¶ï¼‰
            if len(pnl_curve) > 0 and not np.allclose(pnl_curve.values, 0):
                pnl_curves.append(pnl_curve)
                valid_individuals.append(individual)
                print(f"   âœ… å€‹é«” {i+1} (Fitness: {individual.fitness.values[0]:.4f}): Valid PnL curve")
            else:
                print(f"   âš ï¸  å€‹é«” {i+1} (Fitness: {individual.fitness.values[0]:.4f}): Invalid PnL curve (all zeros)")
                
        except Exception as e:
            print(f"   âŒ å€‹é«” {i+1}: Error - {e}")
    
    return pnl_curves, valid_individuals


def calculate_correlation_matrix(pnl_curves: List[pd.Series]) -> np.ndarray:
    """è¨ˆç®— PnL æ›²ç·šçš„ç›¸é—œæ€§çŸ©é™£"""
    n = len(pnl_curves)
    corr_matrix = np.zeros((n, n))
    
    print(f"\nðŸ“ˆ è¨ˆç®—ç›¸é—œæ€§çŸ©é™£ ({n} x {n})...")
    
    for i in range(n):
        for j in range(n):
            if i == j:
                corr_matrix[i, j] = 1.0
            elif i < j:
                # è¨ˆç®— Pearson ç›¸é—œä¿‚æ•¸
                corr = np.corrcoef(pnl_curves[i].values, pnl_curves[j].values)[0, 1]
                corr_matrix[i, j] = corr
                corr_matrix[j, i] = corr
                print(f"   Corr({i+1}, {j+1}) = {corr:.4f}")
    
    return corr_matrix


def visualize_pnl_curves(pnl_curves: List[pd.Series], individuals: List, output_path: Path):
    """è¦–è¦ºåŒ– PnL æ›²ç·š"""
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    # å­åœ– 1: ç´¯ç© PnL
    ax1 = axes[0]
    for i, pnl in enumerate(pnl_curves):
        fitness = individuals[i].fitness.values[0]
        ax1.plot(pnl.index, pnl.values, label=f'Individual {i+1} (Fitness: {fitness:.4f})', linewidth=2, alpha=0.8)
    
    ax1.set_xlabel('Date', fontsize=12)
    ax1.set_ylabel('Cumulative PnL ($)', fontsize=12)
    ax1.set_title('PnL Curves of Selected Individuals', fontsize=14, fontweight='bold')
    ax1.legend(loc='best', fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    
    # å­åœ– 2: æ¨™æº–åŒ– PnL (æ–¹ä¾¿æ¯”è¼ƒå½¢ç‹€)
    ax2 = axes[1]
    for i, pnl in enumerate(pnl_curves):
        # æ¨™æº–åŒ–: (x - mean) / std
        normalized = (pnl - pnl.mean()) / pnl.std()
        fitness = individuals[i].fitness.values[0]
        ax2.plot(normalized.index, normalized.values, label=f'Individual {i+1} (Fitness: {fitness:.4f})', linewidth=2, alpha=0.8)
    
    ax2.set_xlabel('Date', fontsize=12)
    ax2.set_ylabel('Normalized PnL (z-score)', fontsize=12)
    ax2.set_title('Normalized PnL Curves (for Shape Comparison)', fontsize=14, fontweight='bold')
    ax2.legend(loc='best', fontsize=10)
    ax2.grid(True, alpha=0.3)
    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… PnL curves saved: {output_path}")
    plt.close()


def visualize_correlation_matrix(corr_matrix: np.ndarray, individuals: List, output_path: Path):
    """è¦–è¦ºåŒ–ç›¸é—œæ€§çŸ©é™£"""
    n = len(individuals)
    
    # å‰µå»ºæ¨™ç±¤
    labels = [f'Ind {i+1}\n(F:{ind.fitness.values[0]:.3f})' for i, ind in enumerate(individuals)]
    
    # ç¹ªè£½ç†±åœ–
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ä½¿ç”¨ seaborn ç¹ªè£½ç†±åœ–
    sns.heatmap(
        corr_matrix,
        annot=True,
        fmt='.3f',
        cmap='RdYlGn',
        center=0,
        vmin=-1,
        vmax=1,
        square=True,
        linewidths=1,
        cbar_kws={'label': 'Correlation Coefficient'},
        xticklabels=labels,
        yticklabels=labels,
        ax=ax
    )
    
    ax.set_title('PnL Correlation Matrix Between Individuals', fontsize=14, fontweight='bold', pad=20)
    
    # æ·»åŠ çµ±è¨ˆä¿¡æ¯
    upper_tri = np.triu_indices_from(corr_matrix, k=1)
    correlations = corr_matrix[upper_tri]
    
    stats_text = f'Mean: {np.mean(correlations):.3f} | Std: {np.std(correlations):.3f} | Min: {np.min(correlations):.3f} | Max: {np.max(correlations):.3f}'
    plt.figtext(0.5, 0.02, stats_text, ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Correlation matrix saved: {output_path}")
    plt.close()


def main():
    records_dir = Path("test_evolution_records_20251125_0000")
    
    print("=" * 80)
    print("ðŸ§ª PnL Diversity Test")
    print("=" * 80)
    print(f"Records directory: {records_dir}\n")
    
    # 1. è¨­ç½® DEAP
    setup_deap_creator()
    
    # 2. å¾žå¯¦é©—çµæžœç›®éŒ„è¼‰å…¥é…ç½®ï¼ˆä½¿ç”¨å¯¦é©—æ™‚çš„é…ç½®ï¼‰
    config_file = records_dir / "config.json"
    print(f"ðŸ“‹ è¼‰å…¥å¯¦é©—é…ç½®: {config_file}")
    with open(config_file, 'r') as f:
        config = json.load(f)
    
    print(f"   Tickers directory: {config['data']['tickers_dir']}")
    print(f"   Train backtest: {config['data']['train_backtest_start']} to {config['data']['train_backtest_end']}")
    
    # 3. è¼‰å…¥æ•¸æ“šï¼ˆå¾žå¯¦é©—é…ç½®ä¸­çš„ tickers_dirï¼‰
    print(f"\nðŸ“Š è¼‰å…¥æ•¸æ“š...")
    tickers_dir = config['data']['tickers_dir']
    
    # è‡ªå‹•ç™¼ç¾ tickers_dir ä¸­çš„æ‰€æœ‰ CSV æ–‡ä»¶
    import os
    ticker_files = [f for f in os.listdir(tickers_dir) if f.endswith('.csv')]
    tickers = [f.replace('.csv', '') for f in ticker_files]
    print(f"   ç™¼ç¾ {len(tickers)} å€‹ ticker: {tickers[:5]}...")
    
    data = load_and_process_data(tickers_dir, tickers)
    train_data, test_data = split_train_test_data(
        data,
        train_data_start=config['data']['train_data_start'],
        train_backtest_start=config['data']['train_backtest_start'],
        train_backtest_end=config['data']['train_backtest_end'],
        test_data_start=config['data']['test_data_start'],
        test_backtest_start=config['data']['test_backtest_start'],
        test_backtest_end=config['data']['test_backtest_end']
    )
    print(f"âœ… è¼‰å…¥ {len(train_data)} å€‹è‚¡ç¥¨çš„è¨“ç·´æ•¸æ“š\n")
    
    # 4. è¼‰å…¥åˆå§‹ä¸–ä»£çš„å€‹é«”ï¼ˆèˆ‡ best_signals å°æ‡‰ï¼‰
    generation = 0  # ä½¿ç”¨åˆå§‹ä¸–ä»£ï¼Œèˆ‡ best_signals/generation_000 å°æ‡‰
    population = load_individuals_from_generation(records_dir, generation)
    
    # 5. é¸æ“‡ 5 å€‹å¤šæ¨£åŒ–çš„å€‹é«”
    selected_individuals = select_diverse_individuals(population, n=5)
    
    # 6. è¨ˆç®— PnL æ›²ç·š
    backtest_config = {
        'backtest_start': config['data']['train_backtest_start'],
        'backtest_end': config['data']['train_backtest_end']
    }
    pnl_curves, valid_individuals = calculate_pnl_curves(selected_individuals, train_data, backtest_config)
    
    if len(pnl_curves) < 2:
        print("\nâŒ æ²’æœ‰è¶³å¤ çš„æœ‰æ•ˆ PnL æ›²ç·šé€²è¡Œåˆ†æž")
        return
    
    # 7. è¨ˆç®—ç›¸é—œæ€§çŸ©é™£
    corr_matrix = calculate_correlation_matrix(pnl_curves)
    
    # 8. è¦–è¦ºåŒ–
    print("\nðŸŽ¨ ç”Ÿæˆè¦–è¦ºåŒ–...")
    visualize_pnl_curves(pnl_curves, valid_individuals, records_dir / "pnl_curves_comparison.png")
    visualize_correlation_matrix(corr_matrix, valid_individuals, records_dir / "pnl_correlation_matrix.png")
    
    # 9. è¼¸å‡ºçµ±è¨ˆæ‘˜è¦
    print("\n" + "=" * 80)
    print("ðŸ“Š PnL Correlation Statistics")
    print("=" * 80)
    upper_tri = np.triu_indices_from(corr_matrix, k=1)
    correlations = corr_matrix[upper_tri]
    
    print(f"Number of individuals: {len(valid_individuals)}")
    print(f"Number of correlation pairs: {len(correlations)}")
    print(f"Mean correlation: {np.mean(correlations):.4f}")
    print(f"Std correlation: {np.std(correlations):.4f}")
    print(f"Min correlation: {np.min(correlations):.4f}")
    print(f"Max correlation: {np.max(correlations):.4f}")
    print(f"Median correlation: {np.median(correlations):.4f}")
    print("=" * 80)
    print("\nâœ… Test completed!")


if __name__ == "__main__":
    main()
