#!/usr/bin/env python3
"""
çµ„ä»¶åŒ–æ¼”åŒ–è¨ˆç®—æ¡†æ¶ - çµ±ä¸€å…¥å£é»

é€™å€‹å…¥å£é»å¯¦ç¾äº†æ–¹æ¡ˆ Cï¼šçµ„ä»¶åŒ–æ¶æ§‹ï¼Œæä¾›äº†ä¸€å€‹çµ±ä¸€çš„æ¥å£ä¾†é‹è¡Œæ¼”åŒ–å¯¦é©—ã€‚
æ”¯æŒé€šé JSON é…ç½®æ–‡ä»¶ä¾†é…ç½®æ‰€æœ‰æ¼”åŒ–åƒæ•¸å’Œçµ„ä»¶ã€‚

ä½¿ç”¨æ–¹å¼:
    python main_evolution.py --config configs/test_config.json --test
    python main_evolution.py --config configs/portfolio_config.json
"""

import json
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
import copy

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

def load_config(config_path: str) -> Dict[str, Any]:
    """
    è¼‰å…¥é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾‘
        
    Returns:
        é…ç½®å­—å…¸
    """
    print(f"ğŸ“„ è¼‰å…¥é…ç½®æ–‡ä»¶: {config_path}")
    
    config_file = Path(config_path)
    if not config_file.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    print(f"âœ… é…ç½®è¼‰å…¥æˆåŠŸ: {config['experiment']['name']}")
    return config


def load_explicit_data(train_csv: str, test_csv: str) -> Dict[str, Any]:
    """
    å¾æ˜ç¢ºæŒ‡å®šçš„ CSV æª”æ¡ˆè¼‰å…¥è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™ã€‚
    
    ç”¨æ–¼å¤šè³‡ç”¢å¤š fold å¯¦é©—ï¼Œç›´æ¥ä½¿ç”¨ regime_splits çš„ CSV æª”æ¡ˆã€‚
    
    Args:
        train_csv: è¨“ç·´è³‡æ–™ CSV æª”æ¡ˆè·¯å¾‘
        test_csv: æ¸¬è©¦è³‡æ–™ CSV æª”æ¡ˆè·¯å¾‘
        
    Returns:
        è³‡æ–™å­—å…¸ï¼ŒåŒ…å« train_data, test_data, tickers, date_metadata
    """
    import pandas as pd
    
    print(f"ğŸ“Š è¼‰å…¥æ˜ç¢ºæŒ‡å®šçš„è³‡æ–™æª”æ¡ˆ...")
    print(f"   Train: {train_csv}")
    print(f"   Test: {test_csv}")
    
    train_path = Path(train_csv)
    test_path = Path(test_csv)
    
    if not train_path.exists():
        raise FileNotFoundError(f"è¨“ç·´è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {train_csv}")
    if not test_path.exists():
        raise FileNotFoundError(f"æ¸¬è©¦è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {test_csv}")
    
    # å¾æª”åæ¨æ–· ticker åç¨±
    ticker = train_path.parent.name.upper()  # e.g., "btc_usd" -> "BTC_USD"
    
    # è¼‰å…¥è¨“ç·´è³‡æ–™
    train_df = pd.read_csv(train_path, parse_dates=['Date'], index_col='Date')
    if hasattr(train_df.index, 'tz') and train_df.index.tz is not None:
        train_df.index = train_df.index.tz_convert(None)
    train_df.sort_index(inplace=True)
    
    # è¼‰å…¥æ¸¬è©¦è³‡æ–™
    test_df = pd.read_csv(test_path, parse_dates=['Date'], index_col='Date')
    if hasattr(test_df.index, 'tz') and test_df.index.tz is not None:
        test_df.index = test_df.index.tz_convert(None)
    test_df.sort_index(inplace=True)
    
    # å»ºç«‹è³‡æ–™çµæ§‹ï¼ˆèˆ‡ split_train_test_data ç›¸åŒæ ¼å¼ï¼‰
    train_start = train_df.index[0]
    train_end = train_df.index[-1]
    test_start = test_df.index[0]
    test_end = test_df.index[-1]
    
    # è¨ˆç®— warmup æœŸé–“ï¼ˆå‰ 250 å¤©ç”¨æ–¼æŠ€è¡“æŒ‡æ¨™è¨ˆç®—ï¼‰
    warmup_days = min(250, len(train_df) // 4)
    train_backtest_start = train_df.index[warmup_days]
    test_backtest_start = test_df.index[min(warmup_days, len(test_df) // 4)]
    
    train_data = {
        ticker: {
            'data': train_df,
            'backtest_start': str(train_backtest_start.date()),
            'backtest_end': str(train_end.date()),
        }
    }
    
    test_data = {
        ticker: {
            'data': test_df,
            'backtest_start': str(test_backtest_start.date()),
            'backtest_end': str(test_end.date()),
        }
    }
    
    # æ—¥æœŸå…ƒè³‡æ–™ (ç”¨æ–¼æ›´æ–° config)
    date_metadata = {
        'train_data_start': str(train_start.date()),
        'train_backtest_start': str(train_backtest_start.date()),
        'train_backtest_end': str(train_end.date()),
        'test_data_start': str(test_start.date()),
        'test_backtest_start': str(test_backtest_start.date()),
        'test_backtest_end': str(test_end.date()),
    }
    
    print(f"   Ticker: {ticker}")
    print(f"   Train: {len(train_df)} days ({train_start.date()} ~ {train_end.date()})")
    print(f"   Train backtest: {train_backtest_start.date()} ~ {train_end.date()}")
    print(f"   Test: {len(test_df)} days ({test_start.date()} ~ {test_end.date()})")
    print(f"âœ… è³‡æ–™è¼‰å…¥å®Œæˆ")
    
    return {
        'train_data': train_data,
        'test_data': test_data,
        'tickers': [ticker],
        'date_metadata': date_metadata,
    }

def load_portfolio_data(data_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¼‰å…¥æŠ•è³‡çµ„åˆæ•¸æ“š
    
    Args:
        data_config: æ•¸æ“šé…ç½®
        
    Returns:
        è¼‰å…¥çš„æ•¸æ“šå­—å…¸ï¼ŒåŒ…å« train_data, test_data, å’Œå¯é¸çš„ validate_data
    """
    print(f"ğŸ“Š è¼‰å…¥æŠ•è³‡çµ„åˆæ•¸æ“š...")
    
    # ä½¿ç”¨ç¾æœ‰çš„æ•¸æ“šè¼‰å…¥é‚è¼¯
    from gp_quant.data.loader import load_and_process_data, split_train_test_data
    import os
    
    # å¾ TSE300_selected ç›®éŒ„è¼‰å…¥æ•¸æ“š
    tickers_dir = Path(data_config['tickers_dir'])
    if not tickers_dir.exists():
        raise FileNotFoundError(f"æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {tickers_dir}")
    
    # ç²å–æ‰€æœ‰å¯ç”¨çš„è‚¡ç¥¨ä»£ç¢¼
    csv_files = [f for f in os.listdir(tickers_dir) if f.endswith('.csv')]
    tickers = [f.replace('.csv', '') for f in csv_files]
    
    if not tickers:
        raise ValueError(f"åœ¨ {tickers_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶")
    
    print(f"   ç™¼ç¾ {len(tickers)} å€‹è‚¡ç¥¨: {tickers[:3]}{'...' if len(tickers) > 3 else ''}")
    
    # è¼‰å…¥åŸå§‹æ•¸æ“š
    raw_data = load_and_process_data(str(tickers_dir), tickers)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ validate é…ç½®ï¼ˆå‘ä¸‹å…¼å®¹ï¼‰
    has_validate = all([
        data_config.get('validate_data_start'),
        data_config.get('validate_backtest_start'),
        data_config.get('validate_backtest_end')
    ])
    
    # åˆ†å‰²è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦æ•¸æ“š
    train_data, test_data, validate_data = split_train_test_data(
        raw_data,
        train_data_start=data_config['train_data_start'],
        train_backtest_start=data_config['train_backtest_start'],
        train_backtest_end=data_config['train_backtest_end'],
        test_data_start=data_config['test_data_start'],
        test_backtest_start=data_config['test_backtest_start'],
        test_backtest_end=data_config['test_backtest_end'],
        # Optional validate parameters
        validate_data_start=data_config.get('validate_data_start'),
        validate_backtest_start=data_config.get('validate_backtest_start'),
        validate_backtest_end=data_config.get('validate_backtest_end')
    )
    
    data = {
        'train_data': train_data,
        'test_data': test_data,
        'tickers': tickers
    }
    
    # åªæœ‰åœ¨æœ‰ validate é…ç½®æ™‚æ‰åŠ å…¥
    if validate_data:
        data['validate_data'] = validate_data
        print(f"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(tickers)} å€‹è‚¡ç¥¨ (Train + Validate + Test)")
    else:
        print(f"âœ… æ•¸æ“šè¼‰å…¥å®Œæˆ: {len(tickers)} å€‹è‚¡ç¥¨ (Train + Test)")
    
    return data

def print_experiment_info(config: Dict[str, Any]):
    """æ‰“å°å¯¦é©—ä¿¡æ¯"""
    print("\n" + "ğŸš€" * 60)
    print(f"ğŸ§¬ çµ„ä»¶åŒ–æ¼”åŒ–è¨ˆç®—å¯¦é©—")
    print("ğŸš€" * 60)
    print(f"ğŸ“‹ å¯¦é©—åç¨±: {config['experiment']['name']}")
    print(f"ğŸ“ å¯¦é©—æè¿°: {config['experiment']['description']}")
    print(f"ğŸ“Š æ•¸æ“šæ¨¡å¼: {config['data']['mode']}")
    print(f"ğŸ”¢ æ—ç¾¤å¤§å°: {config['evolution']['population_size']}")
    print(f"ğŸ”„ æ¼”åŒ–ä¸–ä»£: {config['evolution']['generations']}")
    print(f"ğŸ¯ é©æ‡‰åº¦å‡½æ•¸: {config['fitness']['function']}")
    print(f"âš¡ æœ€å¤§è™•ç†å™¨: {config['evolution']['max_processors']}")
    print(f"ğŸ“ è¨˜éŒ„ç›®éŒ„: {config['logging']['records_dir']}")
    
    if config['termination']['early_stopping']:
        print(f"ğŸ›‘ æ—©åœæ©Ÿåˆ¶: å•Ÿç”¨ (patience={config['termination']['parameters']['patience']})")
    else:
        print(f"ğŸ›‘ æ—©åœæ©Ÿåˆ¶: åœç”¨")
    
    print("ğŸš€" * 60 + "\n")

def setup_deap_creator():
    """åˆå§‹åŒ– DEAP creator"""
    from deap import creator, base
    
    # æª¢æŸ¥æ˜¯å¦å·²ç¶“å‰µå»º
    if not hasattr(creator, 'FitnessMax'):
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
    
    if not hasattr(creator, 'Individual'):
        from deap import gp
        creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)

def main():
    """ä¸»å‡½æ•¸ - æ¼”åŒ–è¨ˆç®—å…¥å£é»"""
    
    # è¨­ç½® DEAP creator
    setup_deap_creator()
    
    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(
        description='çµ„ä»¶åŒ–æ¼”åŒ–è¨ˆç®—æ¡†æ¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  python main_evolution.py --config configs/test_config.json --test
  python main_evolution.py --config configs/portfolio_config.json
        """
    )
    parser.add_argument('--config', required=True, help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ (è¦†è“‹ç‚ºå°è¦æ¨¡åƒæ•¸)')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡ºæ¨¡å¼')
    parser.add_argument('--no-timestamp', action='store_true', help='ä¸æ·»åŠ æ™‚é–“æµæ°´è™Ÿåˆ°è¨˜éŒ„ç›®éŒ„')
    
    # New arguments for explicit data paths
    parser.add_argument('--train-data', type=str, help='æ˜ç¢ºæŒ‡å®šè¨“ç·´è³‡æ–™ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--test-data', type=str, help='æ˜ç¢ºæŒ‡å®šæ¸¬è©¦è³‡æ–™ CSV æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--output-dir', type=str, help='è¦†è“‹è¼¸å‡ºç›®éŒ„ (logging.records_dir)')
    
    args = parser.parse_args()
    
    try:
        # 1. è¼‰å…¥é…ç½®
        config = load_config(args.config)
        
        # 2. æ¸¬è©¦æ¨¡å¼ï¼šè¦†è“‹åƒæ•¸
        if args.test:
            print("ğŸ§ª æ¸¬è©¦æ¨¡å¼å•Ÿç”¨")
            config['evolution']['population_size'] = 100
            config['evolution']['generations'] = 10
            config['logging']['records_dir'] = 'test_evolution_records'
            config['termination']['parameters']['patience'] = 5
            print(f"   â”œâ”€ æ—ç¾¤å¤§å°: {config['evolution']['population_size']}")
            print(f"   â”œâ”€ æ¼”åŒ–ä¸–ä»£: {config['evolution']['generations']}")
            print(f"   â””â”€ è¨˜éŒ„ç›®éŒ„: {config['logging']['records_dir']}")
        
        # 2.1 è¦†è“‹è¼¸å‡ºç›®éŒ„ (--output-dir)
        if args.output_dir:
            config['logging']['records_dir'] = args.output_dir
            print(f"ğŸ“ ä½¿ç”¨æŒ‡å®šè¼¸å‡ºç›®éŒ„: {args.output_dir}")
        
        # 2.5. æ·»åŠ æ™‚é–“æµæ°´è™Ÿåˆ°è¨˜éŒ„ç›®éŒ„
        if not args.no_timestamp:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M')
            original_dir = config['logging']['records_dir']
            config['logging']['records_dir'] = f"{original_dir}_{timestamp}"
            print(f"ğŸ• æ·»åŠ æ™‚é–“æµæ°´è™Ÿ: {original_dir} -> {config['logging']['records_dir']}")
        
        # 3. æ‰“å°å¯¦é©—ä¿¡æ¯
        print_experiment_info(config)
        
        # 4. å‰µå»ºè¨˜éŒ„ç›®éŒ„
        records_dir = Path(config['logging']['records_dir'])
        records_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ è¨˜éŒ„ç›®éŒ„å·²å‰µå»º: {records_dir}")
        
        # 5. è¼‰å…¥æ•¸æ“š
        if args.train_data and args.test_data:
            # ä½¿ç”¨æ˜ç¢ºæŒ‡å®šçš„ CSV æª”æ¡ˆ
            data = load_explicit_data(args.train_data, args.test_data)
            
            # æ›´æ–° config çš„æ—¥æœŸè¨­å®š (ç”¨æ–¼ Rolling Window Evaluator)
            date_meta = data['date_metadata']
            config['data']['train_data_start'] = date_meta['train_data_start']
            config['data']['train_backtest_start'] = date_meta['train_backtest_start']
            config['data']['train_backtest_end'] = date_meta['train_backtest_end']
            config['data']['test_data_start'] = date_meta['test_data_start']
            config['data']['test_backtest_start'] = date_meta['test_backtest_start']
            config['data']['test_backtest_end'] = date_meta['test_backtest_end']
            print(f"ğŸ“… å·²æ›´æ–° config æ—¥æœŸ: train {date_meta['train_backtest_start']} ~ {date_meta['train_backtest_end']}")
            
            # ç‚º parallel worker å»ºç«‹ symlink (è®“ tickers_dir èƒ½æ‰¾åˆ°è³‡æ–™)
            import os
            import tempfile
            ticker = data['tickers'][0]
            train_path = Path(args.train_data).resolve()
            
            # å»ºç«‹è‡¨æ™‚ç›®éŒ„ä¸¦ symlink è¨“ç·´è³‡æ–™
            temp_tickers_dir = Path(tempfile.mkdtemp(prefix="gp_quant_tickers_"))
            symlink_path = temp_tickers_dir / f"{ticker}.csv"
            symlink_path.symlink_to(train_path)
            
            # æ›´æ–° config çš„ tickers_dir
            config['data']['tickers_dir'] = str(temp_tickers_dir)
            print(f"ğŸ”— å»ºç«‹ symlink: {symlink_path} -> {train_path}")
            print(f"   Parallel mode enabled with tickers_dir: {temp_tickers_dir}")
        elif args.train_data or args.test_data:
            raise ValueError("å¿…é ˆåŒæ™‚æŒ‡å®š --train-data å’Œ --test-data")
        else:
            # ä½¿ç”¨å‚³çµ±çš„è³‡æ–™å¤¾è¼‰å…¥æ–¹å¼
            data = load_portfolio_data(config['data'])
        
        # 6. é¸æ“‡ä¸¦å‰µå»ºå¼•æ“
        experiment_type = config['experiment'].get('type', 'standard')
        
        if experiment_type == 'walk_forward':
            print(f"ğŸ—ï¸ å‰µå»º Walk-Forward æ¼”åŒ–å¼•æ“...")
            from gp_quant.backtesting.walk_forward import WalkForwardEvolutionEngine
            engine = WalkForwardEvolutionEngine(config)
            print(f"âœ… Walk-Forward å¼•æ“å‰µå»ºå®Œæˆ")
            
            print(f"\nğŸš€ é–‹å§‹ Walk-Forward åˆ†æ...")
            start_time = datetime.now()
            
            # WF engine run returns a dict
            wf_result = engine.run(data)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if wf_result:
                print(f"\nâœ… Walk-Forward åˆ†æå®Œæˆ!")
                print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’ ({duration/60:.2f} åˆ†é˜)")
                print(f"ğŸ“ˆ è™•ç†è¦–çª—æ•¸: {len(wf_result['window_results'])}")
                print(f"ğŸ’° ç¸½å›å ±: {wf_result['metrics']['total_return']:.2%}")
                print(f"ğŸ“Š Sharpe Ratio: {wf_result['metrics']['sharpe_ratio']:.4f}")
                print(f"ğŸ“‰ Max Drawdown: {wf_result['metrics']['max_drawdown']:.2%}")
                
                # ä¿å­˜çµæœ
                result_file = Path(config['logging']['records_dir']) / 'final_result.json'
                
                # Convert Series to list/dict for JSON serialization
                # We need to be careful with serialization
                serializable_result = copy.deepcopy(wf_result)
                # Convert equity curve to list of [date, value] or just values
                # Actually, let's just save metrics and window summary for now
                # The equity curve is a Series with DatetimeIndex
                
                # Simple serialization helper
                def convert_for_json(obj):
                    if isinstance(obj, pd.Series):
                        return obj.to_dict() # Index (Timestamp) to value
                    if isinstance(obj, pd.Timestamp):
                        return obj.isoformat()
                    return str(obj)

                # Save full result with custom encoder logic or just simplified
                # Let's save a simplified version
                final_output = {
                    'metrics': wf_result['metrics'],
                    'window_results': []
                }
                
                for wr in wf_result['window_results']:
                    win_res = {
                        'window_index': wr['window_index'],
                        'train_period': f"{wr['window']['train_start'].date()} to {wr['window']['train_end'].date()}",
                        'test_period': f"{wr['window']['test_start'].date()} to {wr['window']['test_end'].date()}",
                        'best_fitness': wr['best_fitness'],
                        'oos_metrics': wr['oos_metrics']
                    }
                    final_output['window_results'].append(win_res)

                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(final_output, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ† æœ€çµ‚çµæœä¿å­˜æ–¼: {result_file}")
                
                # Also save summary
                summary = {
                    'experiment_name': config['experiment']['name'],
                    'type': 'walk_forward',
                    'start_time': start_time.isoformat(),
                    'end_time': end_time.isoformat(),
                    'duration_seconds': duration,
                    'metrics': wf_result['metrics'],
                    'config': config
                }
                summary_file = Path(config['logging']['records_dir']) / 'experiment_summary.json'
                with open(summary_file, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“„ å¯¦é©—æ‘˜è¦ä¿å­˜æ–¼: {summary_file}")
                
                return wf_result
            else:
                print("âŒ Walk-Forward åˆ†ææœªè¿”å›çµæœ")
                return None

        else:
            # Standard Evolution
            print(f"ğŸ—ï¸ å‰µå»ºçµ„ä»¶åŒ–æ¼”åŒ–å¼•æ“...")
            from gp_quant.evolution.components import create_evolution_engine
            
            engine = create_evolution_engine(config)
            print(f"âœ… æ¼”åŒ–å¼•æ“å‰µå»ºå®Œæˆ")
            
            # 7. åŸ·è¡Œæ¼”åŒ–
            print(f"\nğŸš€ é–‹å§‹æ¼”åŒ–è¨ˆç®—...")
            start_time = datetime.now()
            
            result = engine.evolve(data)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # è¨­ç½®åŸ·è¡Œæ™‚é–“
            result.execution_time = duration
            
            # 8. è¼¸å‡ºçµæœ
            print(f"\nâœ… æ¼”åŒ–è¨ˆç®—å®Œæˆ!")
            print(f"â±ï¸  ç¸½åŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’ ({duration/60:.2f} åˆ†é˜)")
            print(f"ğŸ“ˆ æœ€çµ‚ä¸–ä»£: {result.final_generation}")
            print(f"ğŸ† æœ€ä½³é©æ‡‰åº¦: {result.best_fitness:.4f}")
            print(f"ğŸ“ è¨˜éŒ„ä¿å­˜æ–¼: {config['logging']['records_dir']}")
            if result.genealogy:
                print(f"ğŸ§¬ å€‹é«”è­œç³»è¨˜éŒ„: {len(result.genealogy)} å€‹å€‹é«”")
            else:
                print(f"ğŸ§¬ å€‹é«”è­œç³»è¨˜éŒ„: æœªå•Ÿç”¨")
            
            # 9. ä¿å­˜æœ€çµ‚çµæœæ‘˜è¦
            summary = {
                'experiment_name': config['experiment']['name'],
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration,
                'final_generation': result.final_generation,
                'best_fitness': result.best_fitness,
                'population_size': config['evolution']['population_size'],
                'total_individuals_created': len(result.genealogy) if result.genealogy else 0,
                'config': config
            }
            
            summary_file = Path(config['logging']['records_dir']) / 'experiment_summary.json'
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ å¯¦é©—æ‘˜è¦ä¿å­˜æ–¼: {summary_file}")
            
            return result
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ¶ä¸­æ–·å¯¦é©—")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å¯¦é©—åŸ·è¡Œå¤±æ•—: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
