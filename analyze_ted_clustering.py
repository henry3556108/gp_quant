#!/usr/bin/env python3
"""
Analyze TED-based Clustering with Visualization

è¨ˆç®—æŒ‡å®šä¸–ä»£çš„ TED distance matrixï¼Œé€²è¡Œéšå±¤å¼åˆ†ç¾¤ï¼Œä¸¦ä½¿ç”¨ PCA è¦–è¦ºåŒ–ã€‚
"""

import sys
import pickle
import json
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from typing import List, Dict, Any, Tuple
from deap import creator, base, gp
from joblib import Parallel, delayed
from tqdm import tqdm
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import squareform

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from gp_quant.evolution.components.gp import operators
from gp_quant.similarity.tree_edit_distance import compute_ted


def setup_deap_creator():
    """è¨­ç½® DEAP creator"""
    if not hasattr(creator, "FitnessMax"):
        creator.create("FitnessMax", base.Fitness, weights=(1.0,))
    if not hasattr(creator, "Individual"):
        creator.create("Individual", gp.PrimitiveTree, fitness=creator.FitnessMax)


def load_generation_population(records_dir: Path, generation: int) -> List:
    """
    è¼‰å…¥æŒ‡å®šä¸–ä»£çš„æ—ç¾¤
    
    Args:
        records_dir: è¨˜éŒ„ç›®éŒ„è·¯å¾‘
        generation: ä¸–ä»£è™Ÿ
        
    Returns:
        æ—ç¾¤åˆ—è¡¨
    """
    populations_dir = records_dir / 'populations'
    
    if not populations_dir.exists():
        raise ValueError(f"Populations directory not found: {populations_dir}")
    
    gen_file = populations_dir / f'generation_{generation:03d}.pkl'
    
    if not gen_file.exists():
        raise ValueError(f"Generation {generation} file not found: {gen_file}")
    
    print(f"ğŸ“‚ è¼‰å…¥ä¸–ä»£ {generation}: {gen_file.name}")
    
    with open(gen_file, 'rb') as f:
        population = pickle.load(f)
    
    print(f"   âœ… è¼‰å…¥ {len(population)} å€‹å€‹é«”")
    
    return population


def calculate_ted_for_pair(i: int, j: int, ind_i: Any, ind_j: Any) -> Tuple[int, int, float]:
    """
    è¨ˆç®—ä¸€å°å€‹é«”çš„æ¨™æº–åŒ– TED
    
    Args:
        i, j: å€‹é«”ç´¢å¼•
        ind_i, ind_j: å€‹é«”
        
    Returns:
        (i, j, normalized_ted)
    """
    try:
        ted = compute_ted(ind_i, ind_j)
        max_size = max(len(ind_i), len(ind_j))
        norm_ted = ted / max_size if max_size > 0 else 0.0
        return i, j, norm_ted
    except Exception as e:
        # å¦‚æœè¨ˆç®—å¤±æ•—ï¼Œè¿”å›æœ€å¤§è·é›¢
        return i, j, 1.0


def calculate_ted_distance_matrix(population: List, n_jobs: int = 6) -> np.ndarray:
    """
    è¨ˆç®—æ¨™æº–åŒ– TED distance matrixï¼ˆå¹³è¡ŒåŒ–ï¼‰
    
    Args:
        population: æ—ç¾¤åˆ—è¡¨
        n_jobs: å¹³è¡Œè™•ç†å™¨æ•¸é‡
        
    Returns:
        Normalized TED distance matrix (n x n)
    """
    n = len(population)
    print(f"\nğŸŒ³ è¨ˆç®—æ¨™æº–åŒ– TED Distance Matrix ({n} x {n})...")
    
    # åˆå§‹åŒ–çŸ©é™£
    ted_matrix = np.zeros((n, n))
    
    # ç”Ÿæˆæ‰€æœ‰éœ€è¦è¨ˆç®—çš„é…å°ï¼ˆä¸Šä¸‰è§’ï¼‰
    pairs = [(i, j, population[i], population[j]) 
             for i in range(n) for j in range(i + 1, n)]
    
    total_pairs = len(pairs)
    print(f"   ğŸ”„ å¹³è¡Œè¨ˆç®— {total_pairs} å° TED (n_jobs={n_jobs})...")
    
    # å¹³è¡Œè¨ˆç®—ï¼ˆä½¿ç”¨ threading backend é¿å… DEAP creator åºåˆ—åŒ–å•é¡Œï¼‰
    results = Parallel(n_jobs=n_jobs, backend='threading')(
        delayed(calculate_ted_for_pair)(i, j, ind_i, ind_j)
        for i, j, ind_i, ind_j in tqdm(pairs, desc="   è¨ˆç®— TED", ncols=80)
    )
    
    # å¡«å……çŸ©é™£ï¼ˆå°ç¨±ï¼‰
    for i, j, ted in results:
        ted_matrix[i, j] = ted
        ted_matrix[j, i] = ted
    
    # å°è§’ç·šç‚º 0
    np.fill_diagonal(ted_matrix, 0.0)
    
    # çµ±è¨ˆä¿¡æ¯
    upper_tri = np.triu_indices(n, k=1)
    mean_ted = np.mean(ted_matrix[upper_tri])
    std_ted = np.std(ted_matrix[upper_tri])
    
    print(f"   âœ… å¹³å‡ TED è·é›¢: {mean_ted:.4f} Â± {std_ted:.4f}")
    print(f"   âœ… TED ç¯„åœ: [{np.min(ted_matrix[upper_tri]):.4f}, {np.max(ted_matrix[upper_tri]):.4f}]")
    
    return ted_matrix


def perform_hierarchical_clustering(distance_matrix: np.ndarray, 
                                     n_clusters: int = 3) -> Tuple[np.ndarray, Any]:
    """
    åŸ·è¡Œéšå±¤å¼åˆ†ç¾¤
    
    Args:
        distance_matrix: è·é›¢çŸ©é™£
        n_clusters: ç¾¤æ•¸
        
    Returns:
        (cluster_labels, linkage_matrix)
    """
    print(f"\nğŸ”¬ åŸ·è¡Œéšå±¤å¼åˆ†ç¾¤ (K={n_clusters})...")
    
    # ä½¿ç”¨ AgglomerativeClustering
    clustering = AgglomerativeClustering(
        n_clusters=n_clusters,
        metric='precomputed',
        linkage='complete'  # ä½¿ç”¨ complete linkage
    )
    
    cluster_labels = clustering.fit_predict(distance_matrix)
    
    # è¨ˆç®— linkage matrix ç”¨æ–¼ dendrogram
    # å°‡è·é›¢çŸ©é™£è½‰æ›ç‚ºå£“ç¸®å½¢å¼
    condensed_dist = squareform(distance_matrix)
    linkage_matrix = linkage(condensed_dist, method='average')
    
    # çµ±è¨ˆæ¯å€‹ç¾¤çš„å¤§å°
    unique_labels, counts = np.unique(cluster_labels, return_counts=True)
    print(f"   âœ… åˆ†ç¾¤å®Œæˆ:")
    for label, count in zip(unique_labels, counts):
        print(f"      ç¾¤ {label}: {count} å€‹å€‹é«”")
    
    return cluster_labels, linkage_matrix


def visualize_clustering_pca(distance_matrix: np.ndarray,
                              cluster_labels: np.ndarray,
                              population: List,
                              output_path: Path):
    """
    ä½¿ç”¨ PCA å°‡è·é›¢çŸ©é™£é™ç¶­åˆ° 2D ä¸¦è¦–è¦ºåŒ–åˆ†ç¾¤çµæœ
    
    Args:
        distance_matrix: è·é›¢çŸ©é™£
        cluster_labels: ç¾¤æ¨™ç±¤
        population: æ—ç¾¤åˆ—è¡¨
        output_path: è¼¸å‡ºåœ–è¡¨è·¯å¾‘
    """
    print(f"\nğŸ“Š ä½¿ç”¨ PCA é™ç¶­ä¸¦è¦–è¦ºåŒ–...")
    
    # ä½¿ç”¨ PCA é™ç¶­ï¼ˆå¾è·é›¢çŸ©é™£ï¼‰
    # æ³¨æ„ï¼šPCA é€šå¸¸ç”¨æ–¼ç‰¹å¾µçŸ©é™£ï¼Œä½†æˆ‘å€‘å¯ä»¥ç”¨è·é›¢çŸ©é™£çš„ MDS æ•ˆæœ
    # é€™è£¡ä½¿ç”¨ç°¡å–®çš„ PCA ä½œç‚ºç¤ºç¯„
    pca = PCA(n_components=2)
    coords_2d = pca.fit_transform(distance_matrix)
    
    print(f"   âœ… PCA è§£é‡‹è®Šç•°é‡: {pca.explained_variance_ratio_}")
    print(f"   âœ… ç´¯ç©è§£é‡‹è®Šç•°é‡: {sum(pca.explained_variance_ratio_):.4f}")
    
    # å‰µå»ºåœ–è¡¨
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    
    # ========== å·¦åœ–ï¼šPCA æ•£é»åœ– ==========
    ax_scatter = axes[0]
    
    # ç‚ºæ¯å€‹ç¾¤ä½¿ç”¨ä¸åŒé¡è‰²
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E']
    markers = ['o', 's', '^', 'D', 'v']
    
    unique_labels = np.unique(cluster_labels)
    
    for label in unique_labels:
        mask = cluster_labels == label
        ax_scatter.scatter(
            coords_2d[mask, 0],
            coords_2d[mask, 1],
            c=colors[label % len(colors)],
            marker=markers[label % len(markers)],
            s=100,
            alpha=0.6,
            label=f'Cluster {label} (n={np.sum(mask)})',
            edgecolors='black',
            linewidth=0.5
        )
    
    ax_scatter.set_xlabel('PC1', fontsize=13, fontweight='bold')
    ax_scatter.set_ylabel('PC2', fontsize=13, fontweight='bold')
    ax_scatter.set_title('TED-based Clustering (PCA Visualization)', 
                         fontsize=15, fontweight='bold', pad=15)
    ax_scatter.legend(loc='best', fontsize=11, framealpha=0.9)
    ax_scatter.grid(True, alpha=0.3, linestyle='--')
    
    # æ·»åŠ çµ±è¨ˆä¿¡æ¯
    stats_text = f'Total Individuals: {len(population)} | Clusters: {len(unique_labels)}'
    ax_scatter.text(0.5, 0.02, stats_text, transform=ax_scatter.transAxes,
                   ha='center', fontsize=10,
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))
    
    # ========== å³åœ–ï¼šæ¯å€‹ç¾¤çš„ Fitness åˆ†å¸ƒ ==========
    ax_fitness = axes[1]
    
    fitness_by_cluster = []
    for label in unique_labels:
        mask = cluster_labels == label
        cluster_fitness = [population[i].fitness.values[0] for i in range(len(population)) if mask[i]]
        fitness_by_cluster.append(cluster_fitness)
    
    # ç¹ªè£½ç®±å‹åœ–
    bp = ax_fitness.boxplot(fitness_by_cluster, 
                            labels=[f'Cluster {i}' for i in unique_labels],
                            patch_artist=True,
                            notch=True,
                            showmeans=True)
    
    # è¨­ç½®é¡è‰²
    for patch, label in zip(bp['boxes'], unique_labels):
        patch.set_facecolor(colors[label % len(colors)])
        patch.set_alpha(0.6)
    
    ax_fitness.set_xlabel('Cluster', fontsize=13, fontweight='bold')
    ax_fitness.set_ylabel('Fitness', fontsize=13, fontweight='bold')
    ax_fitness.set_title('Fitness Distribution by Cluster', 
                        fontsize=15, fontweight='bold', pad=15)
    ax_fitness.grid(True, alpha=0.3, linestyle='--', axis='y')
    
    # æ·»åŠ çµ±è¨ˆä¿¡æ¯
    for i, (label, fitness_list) in enumerate(zip(unique_labels, fitness_by_cluster)):
        mean_fitness = np.mean(fitness_list)
        std_fitness = np.std(fitness_list)
        ax_fitness.text(i + 1, ax_fitness.get_ylim()[1] * 0.95, 
                       f'Î¼={mean_fitness:.4f}\nÏƒ={std_fitness:.4f}',
                       ha='center', fontsize=9,
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… åœ–è¡¨å·²ä¿å­˜: {output_path}")
    plt.close()


def save_clustering_results(distance_matrix: np.ndarray,
                            cluster_labels: np.ndarray,
                            population: List,
                            output_dir: Path,
                            generation: int):
    """
    ä¿å­˜åˆ†ç¾¤çµæœ
    
    Args:
        distance_matrix: è·é›¢çŸ©é™£
        cluster_labels: ç¾¤æ¨™ç±¤
        population: æ—ç¾¤åˆ—è¡¨
        output_dir: è¼¸å‡ºç›®éŒ„
        generation: ä¸–ä»£è™Ÿ
    """
    print(f"\nğŸ’¾ ä¿å­˜åˆ†ç¾¤çµæœ...")
    
    # 1. ä¿å­˜è·é›¢çŸ©é™£
    dist_path = output_dir / f'ted_distance_matrix_gen{generation:03d}.csv'
    pd.DataFrame(distance_matrix).to_csv(dist_path, index=False, header=False)
    print(f"   âœ… è·é›¢çŸ©é™£å·²ä¿å­˜: {dist_path}")
    
    # 2. ä¿å­˜ç¾¤æ¨™ç±¤å’Œå€‹é«”ä¿¡æ¯
    cluster_info = []
    for i, (individual, label) in enumerate(zip(population, cluster_labels)):
        cluster_info.append({
            'individual_id': i,
            'cluster': int(label),
            'fitness': individual.fitness.values[0],
            'tree_size': len(individual),
            'tree_depth': individual.height
        })
    
    cluster_df = pd.DataFrame(cluster_info)
    cluster_path = output_dir / f'cluster_assignments_gen{generation:03d}.csv'
    cluster_df.to_csv(cluster_path, index=False)
    print(f"   âœ… ç¾¤æ¨™ç±¤å·²ä¿å­˜: {cluster_path}")
    
    # 3. ä¿å­˜æ¯å€‹ç¾¤çš„çµ±è¨ˆæ‘˜è¦
    summary = []
    for label in np.unique(cluster_labels):
        mask = cluster_labels == label
        cluster_individuals = [population[i] for i in range(len(population)) if mask[i]]
        
        fitnesses = [ind.fitness.values[0] for ind in cluster_individuals]
        tree_sizes = [len(ind) for ind in cluster_individuals]
        tree_depths = [ind.height for ind in cluster_individuals]
        
        summary.append({
            'cluster': int(label),
            'size': int(np.sum(mask)),
            'mean_fitness': np.mean(fitnesses),
            'std_fitness': np.std(fitnesses),
            'min_fitness': np.min(fitnesses),
            'max_fitness': np.max(fitnesses),
            'mean_tree_size': np.mean(tree_sizes),
            'mean_tree_depth': np.mean(tree_depths)
        })
    
    summary_df = pd.DataFrame(summary)
    summary_path = output_dir / f'cluster_summary_gen{generation:03d}.csv'
    summary_df.to_csv(summary_path, index=False)
    print(f"   âœ… ç¾¤çµ±è¨ˆæ‘˜è¦å·²ä¿å­˜: {summary_path}")
    
    return cluster_df, summary_df


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æ TED-based åˆ†ç¾¤ä¸¦è¦–è¦ºåŒ–"
    )
    parser.add_argument(
        '--records',
        type=str,
        required=True,
        help='å¯¦é©—è¨˜éŒ„ç›®éŒ„è·¯å¾‘'
    )
    parser.add_argument(
        '--generation',
        type=int,
        default=0,
        help='ä¸–ä»£è™Ÿï¼ˆé»˜èªï¼š0ï¼‰'
    )
    parser.add_argument(
        '--n-clusters',
        type=int,
        default=3,
        help='ç¾¤æ•¸ï¼ˆé»˜èªï¼š3ï¼‰'
    )
    parser.add_argument(
        '--n-jobs',
        type=int,
        default=6,
        help='å¹³è¡Œè™•ç†å™¨æ•¸é‡ï¼ˆé»˜èªï¼š6ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='è¼¸å‡ºç›®éŒ„ï¼ˆé»˜èªä¿å­˜åœ¨è¨˜éŒ„ç›®éŒ„ä¸­ï¼‰'
    )
    
    args = parser.parse_args()
    
    records_dir = Path(args.records)
    
    if not records_dir.exists():
        print(f"âŒ è¨˜éŒ„ç›®éŒ„ä¸å­˜åœ¨: {records_dir}")
        return
    
    print("=" * 80)
    print("ğŸ¯ TED-based Clustering Analysis")
    print("=" * 80)
    print(f"Records directory: {records_dir}")
    print(f"Generation: {args.generation}")
    print(f"Number of clusters: {args.n_clusters}")
    print(f"N jobs: {args.n_jobs}\n")
    
    # 1. è¨­ç½® DEAP
    setup_deap_creator()
    
    # 2. è¼‰å…¥ä¸–ä»£æ—ç¾¤
    print("ğŸ“¦ è¼‰å…¥ä¸–ä»£æ—ç¾¤...")
    population = load_generation_population(records_dir, args.generation)
    
    # 3. è¨ˆç®— TED distance matrix
    ted_matrix = calculate_ted_distance_matrix(population, args.n_jobs)
    
    # 4. åŸ·è¡Œéšå±¤å¼åˆ†ç¾¤
    cluster_labels, linkage_matrix = perform_hierarchical_clustering(
        ted_matrix, args.n_clusters
    )
    
    # 5. è¨­ç½®è¼¸å‡ºç›®éŒ„
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = records_dir
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 6. è¦–è¦ºåŒ–
    viz_path = output_dir / f'ted_clustering_gen{args.generation:03d}.png'
    visualize_clustering_pca(ted_matrix, cluster_labels, population, viz_path)
    
    # 7. ä¿å­˜çµæœ
    cluster_df, summary_df = save_clustering_results(
        ted_matrix, cluster_labels, population, output_dir, args.generation
    )
    
    # 8. è¼¸å‡ºæ‘˜è¦
    print("\n" + "=" * 80)
    print("âœ… å®Œæˆ!")
    print("=" * 80)
    print(f"ä¸–ä»£: {args.generation}")
    print(f"å€‹é«”æ•¸é‡: {len(population)}")
    print(f"ç¾¤æ•¸: {args.n_clusters}")
    print(f"\nç¾¤çµ±è¨ˆ:")
    print(summary_df.to_string(index=False))
    print(f"\nè¼¸å‡ºæ–‡ä»¶:")
    print(f"  - {viz_path}")
    print(f"  - {output_dir / f'ted_distance_matrix_gen{args.generation:03d}.csv'}")
    print(f"  - {output_dir / f'cluster_assignments_gen{args.generation:03d}.csv'}")
    print(f"  - {output_dir / f'cluster_summary_gen{args.generation:03d}.csv'}")
    print("=" * 80)


if __name__ == "__main__":
    main()
